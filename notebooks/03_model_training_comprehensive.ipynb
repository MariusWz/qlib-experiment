{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Model Training and Optimization\n",
    "# 第三章：模型训练与优化\n",
    "\n",
    "**Author**: Microsoft Qlib Team  \n",
    "**License**: MIT License  \n",
    "**Last Updated**: 2025-01-09\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Table of Contents / 目录\n",
    "\n",
    "### Part 1: Model Fundamentals / 模型基础\n",
    "1. [Model Interface and Architecture / 模型接口与架构](#model-interface)\n",
    "2. [Dataset Preparation / 数据集准备](#dataset-prep)\n",
    "3. [Training Pipeline / 训练管道](#training-pipeline)\n",
    "\n",
    "### Part 2: Tree-Based Models / 树模型\n",
    "4. [LightGBM Models / LightGBM模型](#lightgbm)\n",
    "5. [XGBoost Models / XGBoost模型](#xgboost)\n",
    "6. [CatBoost Models / CatBoost模型](#catboost)\n",
    "7. [Tree Model Comparison / 树模型对比](#tree-comparison)\n",
    "\n",
    "### Part 3: Deep Learning Models / 深度学习模型\n",
    "8. [Neural Network Basics / 神经网络基础](#nn-basics)\n",
    "9. [MLP for Stock Prediction / 用于股票预测的MLP](#mlp)\n",
    "10. [LSTM and GRU Models / LSTM和GRU模型](#lstm-gru)\n",
    "11. [Transformer Models / Transformer模型](#transformer)\n",
    "12. [Graph Neural Networks / 图神经网络](#gnn)\n",
    "\n",
    "### Part 4: Model Optimization / 模型优化\n",
    "13. [Hyperparameter Tuning / 超参数调优](#hyperparameter)\n",
    "14. [Feature Selection / 特征选择](#feature-selection)\n",
    "15. [Ensemble Methods / 集成方法](#ensemble)\n",
    "16. [Cross-Validation Strategies / 交叉验证策略](#cross-validation)\n",
    "\n",
    "### Part 5: Production Deployment / 生产部署\n",
    "17. [Model Persistence / 模型持久化](#persistence)\n",
    "18. [Online Learning / 在线学习](#online-learning)\n",
    "19. [Model Monitoring / 模型监控](#monitoring)\n",
    "20. [Best Practices / 最佳实践](#best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports / 设置和导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports / 必要导入\n",
    "import qlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from datetime import datetime\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Qlib imports / Qlib导入\n",
    "from qlib.data import D\n",
    "from qlib.config import C\n",
    "from qlib.workflow import R\n",
    "from qlib.utils import init_instance_by_config\n",
    "from qlib.model.base import Model\n",
    "from qlib.data.dataset import DatasetH, TSDatasetH\n",
    "\n",
    "# Model imports / 模型导入\n",
    "from qlib.contrib.model.gbdt import LGBModel\n",
    "from qlib.contrib.model.xgboost import XGBModel\n",
    "from qlib.contrib.model.catboost import CatBoostModel\n",
    "from qlib.contrib.model.pytorch_nn import DNNModel\n",
    "from qlib.contrib.model.pytorch_lstm import LSTMModel\n",
    "from qlib.contrib.model.pytorch_gru import GRUModel\n",
    "\n",
    "# Visualization settings / 可视化设置\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Qlib / 初始化Qlib\n",
    "qlib.init()\n",
    "\n",
    "# Global configuration / 全局配置\n",
    "MARKET = \"csi300\"  # 沪深300\n",
    "BENCHMARK = \"SH000300\"  # 基准指数\n",
    "EXP_NAME = \"model_training_exp\"  # 实验名称\n",
    "\n",
    "# Time periods / 时间段\n",
    "TRAIN_START = \"2008-01-01\"\n",
    "TRAIN_END = \"2014-12-31\"\n",
    "VALID_START = \"2015-01-01\"\n",
    "VALID_END = \"2016-12-31\"\n",
    "TEST_START = \"2017-01-01\"\n",
    "TEST_END = \"2020-08-01\"\n",
    "\n",
    "print(f\"Market: {MARKET}\")\n",
    "print(f\"Training: {TRAIN_START} to {TRAIN_END}\")\n",
    "print(f\"Validation: {VALID_START} to {VALID_END}\")\n",
    "print(f\"Testing: {TEST_START} to {TEST_END}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Model Fundamentals / 模型基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Interface and Architecture / 模型接口与架构 <a id='model-interface'></a>\n",
    "\n",
    "### Qlib Model Architecture / Qlib模型架构\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Qlib Model Interface                     │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│   Base Model Class                                         │\n",
    "│   ├── fit(dataset)          # Training / 训练             │\n",
    "│   ├── predict(dataset)      # Prediction / 预测           │\n",
    "│   └── finetune(dataset)     # Fine-tuning / 微调         │\n",
    "│                                                             │\n",
    "│   Model Types / 模型类型:                                  │\n",
    "│   ├── Tabular Models        # Traditional ML / 传统机器学习 │\n",
    "│   ├── Time Series Models    # Sequential / 序列模型        │\n",
    "│   └── Graph Models          # Relational / 关系模型        │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model base class / 自定义模型基类\n",
    "\n",
    "from qlib.model.base import Model\n",
    "from qlib.data.dataset.handler import DataHandlerLP\n",
    "\n",
    "class CustomModelBase(Model):\n",
    "    \"\"\"Custom base model class with enhanced features\n",
    "    带增强功能的自定义基础模型类\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fitted = False\n",
    "        self.training_history = []\n",
    "        self.feature_importance = None\n",
    "    \n",
    "    def fit(self, dataset: DatasetH, **kwargs):\n",
    "        \"\"\"Enhanced fit method with logging\n",
    "        带日志记录的增强训练方法\n",
    "        \"\"\"\n",
    "        print(f\"Starting training at {datetime.now()}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get data\n",
    "        df_train, df_valid = self._prepare_data(dataset)\n",
    "        \n",
    "        # Actual training (to be implemented in subclasses)\n",
    "        self._fit_model(df_train, df_valid, **kwargs)\n",
    "        \n",
    "        # Record training time\n",
    "        training_time = time.time() - start_time\n",
    "        self.training_history.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'training_time': training_time,\n",
    "            'train_samples': len(df_train),\n",
    "            'valid_samples': len(df_valid) if df_valid is not None else 0\n",
    "        })\n",
    "        \n",
    "        self.fitted = True\n",
    "        print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    def predict(self, dataset: DatasetH, segment: str = \"test\"):\n",
    "        \"\"\"Enhanced predict method\n",
    "        增强预测方法\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model must be fitted before prediction\")\n",
    "        \n",
    "        # Get test data\n",
    "        df_test = dataset.prepare(segment, col_set=['feature', 'label'])\n",
    "        \n",
    "        # Make predictions (to be implemented in subclasses)\n",
    "        predictions = self._predict_model(df_test)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def _prepare_data(self, dataset: DatasetH):\n",
    "        \"\"\"Prepare training and validation data\n",
    "        准备训练和验证数据\n",
    "        \"\"\"\n",
    "        df_train = dataset.prepare(\"train\", col_set=[\"feature\", \"label\"])\n",
    "        df_valid = dataset.prepare(\"valid\", col_set=[\"feature\", \"label\"]) if \"valid\" in dataset.segments else None\n",
    "        return df_train, df_valid\n",
    "    \n",
    "    def _fit_model(self, df_train, df_valid, **kwargs):\n",
    "        \"\"\"To be implemented by subclasses\n",
    "        由子类实现\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _predict_model(self, df_test):\n",
    "        \"\"\"To be implemented by subclasses\n",
    "        由子类实现\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Get feature importance if available\n",
    "        获取特征重要性（如果可用）\n",
    "        \"\"\"\n",
    "        return self.feature_importance\n",
    "\n",
    "print(\"✅ Custom model base class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation / 数据集准备 <a id='dataset-prep'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare comprehensive dataset / 准备综合数据集\n",
    "\n",
    "from qlib.contrib.data.handler import Alpha158\n",
    "\n",
    "# Create data handler / 创建数据处理器\n",
    "handler_config = {\n",
    "    \"start_time\": TRAIN_START,\n",
    "    \"end_time\": TEST_END,\n",
    "    \"fit_start_time\": TRAIN_START,\n",
    "    \"fit_end_time\": TRAIN_END,\n",
    "    \"instruments\": MARKET,\n",
    "}\n",
    "\n",
    "# Initialize Alpha158 handler / 初始化Alpha158处理器\n",
    "handler = Alpha158(**handler_config)\n",
    "\n",
    "# Create dataset with segments / 创建带分段的数据集\n",
    "dataset_config = {\n",
    "    \"handler\": handler,\n",
    "    \"segments\": {\n",
    "        \"train\": (TRAIN_START, TRAIN_END),\n",
    "        \"valid\": (VALID_START, VALID_END),\n",
    "        \"test\": (TEST_START, TEST_END),\n",
    "    },\n",
    "}\n",
    "\n",
    "dataset = DatasetH(**dataset_config)\n",
    "\n",
    "# Display dataset information / 显示数据集信息\n",
    "print(\"Dataset segments:\")\n",
    "for segment, (start, end) in dataset.segments.items():\n",
    "    df_segment = dataset.prepare(segment, col_set=['feature', 'label'], data_key=DataHandlerLP.DK_L)\n",
    "    print(f\"  {segment:10}: {start} to {end}, Shape: {df_segment.shape}\")\n",
    "\n",
    "# Sample data preview / 数据样本预览\n",
    "df_sample = dataset.prepare(\"train\", col_set=['feature', 'label'], data_key=DataHandlerLP.DK_L).head()\n",
    "print(f\"\\nFeature columns: {df_sample.columns[:10].tolist()}...\")\n",
    "print(f\"Total features: {len(df_sample.columns) - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Pipeline / 训练管道 <a id='training-pipeline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive training pipeline / 综合训练管道\n",
    "\n",
    "class TrainingPipeline:\n",
    "    \"\"\"Complete training pipeline with experiment tracking\n",
    "    带实验跟踪的完整训练管道\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name: str):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def add_model(self, name: str, model: Model):\n",
    "        \"\"\"Add a model to the pipeline\n",
    "        向管道添加模型\n",
    "        \"\"\"\n",
    "        self.models[name] = model\n",
    "        print(f\"Added model: {name}\")\n",
    "    \n",
    "    def train_all(self, dataset: DatasetH, save_models: bool = True):\n",
    "        \"\"\"Train all models in the pipeline\n",
    "        训练管道中的所有模型\n",
    "        \"\"\"\n",
    "        print(f\"\\nStarting training pipeline with {len(self.models)} models\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            # Start experiment\n",
    "            with R.start(experiment_name=self.experiment_name):\n",
    "                # Train model\n",
    "                start_time = time.time()\n",
    "                model.fit(dataset)\n",
    "                training_time = time.time() - start_time\n",
    "                \n",
    "                # Make predictions\n",
    "                pred_train = model.predict(dataset, segment=\"train\")\n",
    "                pred_valid = model.predict(dataset, segment=\"valid\")\n",
    "                pred_test = model.predict(dataset, segment=\"test\")\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = self._calculate_metrics(dataset, pred_train, pred_valid, pred_test)\n",
    "                \n",
    "                # Store results\n",
    "                self.results[name] = {\n",
    "                    'model': model,\n",
    "                    'predictions': {\n",
    "                        'train': pred_train,\n",
    "                        'valid': pred_valid,\n",
    "                        'test': pred_test\n",
    "                    },\n",
    "                    'metrics': metrics,\n",
    "                    'training_time': training_time\n",
    "                }\n",
    "                \n",
    "                # Save model if requested\n",
    "                if save_models:\n",
    "                    R.save_objects(\n",
    "                        trained_model=model,\n",
    "                        predictions=pred_test,\n",
    "                        metrics=metrics\n",
    "                    )\n",
    "                \n",
    "                print(f\"  ✅ Training completed in {training_time:.2f}s\")\n",
    "                print(f\"  Test IC: {metrics['test_ic']:.4f}\")\n",
    "                print(f\"  Test Rank IC: {metrics['test_rank_ic']:.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return self.results\n",
    "    \n",
    "    def _calculate_metrics(self, dataset, pred_train, pred_valid, pred_test):\n",
    "        \"\"\"Calculate evaluation metrics\n",
    "        计算评估指标\n",
    "        \"\"\"\n",
    "        from scipy.stats import spearmanr\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Get labels\n",
    "        label_train = dataset.prepare(\"train\", col_set=['label'], data_key=DataHandlerLP.DK_L)\n",
    "        label_valid = dataset.prepare(\"valid\", col_set=['label'], data_key=DataHandlerLP.DK_L)\n",
    "        label_test = dataset.prepare(\"test\", col_set=['label'], data_key=DataHandlerLP.DK_L)\n",
    "        \n",
    "        # Align predictions with labels\n",
    "        def calc_ic(pred, label):\n",
    "            df = pd.DataFrame({'pred': pred, 'label': label['label']}).dropna()\n",
    "            if len(df) > 0:\n",
    "                ic = df['pred'].corr(df['label'])\n",
    "                rank_ic = spearmanr(df['pred'], df['label'])[0]\n",
    "                return ic, rank_ic\n",
    "            return 0, 0\n",
    "        \n",
    "        # Calculate metrics for each segment\n",
    "        metrics['train_ic'], metrics['train_rank_ic'] = calc_ic(pred_train, label_train)\n",
    "        metrics['valid_ic'], metrics['valid_rank_ic'] = calc_ic(pred_valid, label_valid)\n",
    "        metrics['test_ic'], metrics['test_rank_ic'] = calc_ic(pred_test, label_test)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare all trained models\n",
    "        比较所有训练过的模型\n",
    "        \"\"\"\n",
    "        comparison_df = pd.DataFrame()\n",
    "        \n",
    "        for name, result in self.results.items():\n",
    "            metrics = result['metrics']\n",
    "            row = {\n",
    "                'Model': name,\n",
    "                'Train IC': metrics['train_ic'],\n",
    "                'Valid IC': metrics['valid_ic'],\n",
    "                'Test IC': metrics['test_ic'],\n",
    "                'Test Rank IC': metrics['test_rank_ic'],\n",
    "                'Training Time (s)': result['training_time']\n",
    "            }\n",
    "            comparison_df = pd.concat([comparison_df, pd.DataFrame([row])], ignore_index=True)\n",
    "        \n",
    "        return comparison_df.sort_values('Test IC', ascending=False)\n",
    "\n",
    "# Create pipeline / 创建管道\n",
    "pipeline = TrainingPipeline(experiment_name=EXP_NAME)\n",
    "print(\"✅ Training pipeline created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tree-Based Models / 树模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LightGBM Models / LightGBM模型 <a id='lightgbm'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM model configuration and training / LightGBM模型配置和训练\n",
    "\n",
    "from qlib.contrib.model.gbdt import LGBModel\n",
    "\n",
    "# Basic LightGBM configuration / 基础LightGBM配置\n",
    "lgb_basic_config = {\n",
    "    \"loss\": \"mse\",\n",
    "    \"colsample_bytree\": 0.8879,\n",
    "    \"learning_rate\": 0.0421,\n",
    "    \"subsample\": 0.8789,\n",
    "    \"lambda_l1\": 205.6999,\n",
    "    \"lambda_l2\": 580.9768,\n",
    "    \"max_depth\": 8,\n",
    "    \"num_leaves\": 210,\n",
    "    \"num_threads\": 20,\n",
    "    \"verbosity\": -1,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "}\n",
    "\n",
    "# Advanced LightGBM configuration / 高级LightGBM配置\n",
    "lgb_advanced_config = {\n",
    "    \"loss\": \"mse\",\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_boost_round\": 1000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 256,\n",
    "    \"max_depth\": 10,\n",
    "    \"min_child_samples\": 20,\n",
    "    \"subsample\": 0.9,\n",
    "    \"subsample_freq\": 1,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"reg_alpha\": 100.0,\n",
    "    \"reg_lambda\": 100.0,\n",
    "    \"seed\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "    \"silent\": True,\n",
    "    \"importance_type\": \"gain\",\n",
    "    \"early_stopping_rounds\": 100,\n",
    "}\n",
    "\n",
    "# Create LightGBM models / 创建LightGBM模型\n",
    "lgb_basic = LGBModel(**lgb_basic_config)\n",
    "lgb_advanced = LGBModel(**lgb_advanced_config)\n",
    "\n",
    "# Add to pipeline / 添加到管道\n",
    "pipeline.add_model(\"LightGBM_Basic\", lgb_basic)\n",
    "pipeline.add_model(\"LightGBM_Advanced\", lgb_advanced)\n",
    "\n",
    "print(\"LightGBM configurations:\")\n",
    "print(f\"  Basic: {len(lgb_basic_config)} parameters\")\n",
    "print(f\"  Advanced: {len(lgb_advanced_config)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom LightGBM with feature engineering / 带特征工程的自定义LightGBM\n",
    "\n",
    "class CustomLGBModel(LGBModel):\n",
    "    \"\"\"LightGBM with custom feature engineering\n",
    "    带自定义特征工程的LightGBM\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_engineer=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.feature_engineer = feature_engineer\n",
    "        self.feature_names = None\n",
    "    \n",
    "    def fit(self, dataset, **kwargs):\n",
    "        \"\"\"Fit with feature engineering\n",
    "        带特征工程的训练\n",
    "        \"\"\"\n",
    "        # Apply feature engineering if provided\n",
    "        if self.feature_engineer:\n",
    "            dataset = self._apply_feature_engineering(dataset)\n",
    "        \n",
    "        # Call parent fit method\n",
    "        super().fit(dataset, **kwargs)\n",
    "        \n",
    "        # Extract feature importance\n",
    "        self._extract_feature_importance()\n",
    "    \n",
    "    def _apply_feature_engineering(self, dataset):\n",
    "        \"\"\"Apply custom feature engineering\n",
    "        应用自定义特征工程\n",
    "        \"\"\"\n",
    "        # Example: Add rolling features\n",
    "        print(\"Applying feature engineering...\")\n",
    "        # Implementation would go here\n",
    "        return dataset\n",
    "    \n",
    "    def _extract_feature_importance(self):\n",
    "        \"\"\"Extract and store feature importance\n",
    "        提取并存储特征重要性\n",
    "        \"\"\"\n",
    "        if hasattr(self.model, 'feature_importance'):\n",
    "            importance = self.model.feature_importance(importance_type='gain')\n",
    "            if self.feature_names:\n",
    "                self.feature_importance = dict(zip(self.feature_names, importance))\n",
    "            else:\n",
    "                self.feature_importance = importance\n",
    "    \n",
    "    def get_top_features(self, n=20):\n",
    "        \"\"\"Get top n important features\n",
    "        获取前n个重要特征\n",
    "        \"\"\"\n",
    "        if self.feature_importance and isinstance(self.feature_importance, dict):\n",
    "            sorted_features = sorted(self.feature_importance.items(), \n",
    "                                   key=lambda x: x[1], reverse=True)\n",
    "            return sorted_features[:n]\n",
    "        return None\n",
    "\n",
    "# Create custom LightGBM model / 创建自定义LightGBM模型\n",
    "custom_lgb = CustomLGBModel(**lgb_basic_config)\n",
    "pipeline.add_model(\"LightGBM_Custom\", custom_lgb)\n",
    "\n",
    "print(\"✅ Custom LightGBM model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBoost Models / XGBoost模型 <a id='xgboost'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model configuration / XGBoost模型配置\n",
    "\n",
    "from qlib.contrib.model.xgboost import XGBModel\n",
    "\n",
    "# XGBoost configuration / XGBoost配置\n",
    "xgb_config = {\n",
    "    \"n_estimators\": 1000,\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"reg_alpha\": 100,\n",
    "    \"reg_lambda\": 100,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"verbosity\": 0,\n",
    "}\n",
    "\n",
    "# GPU-accelerated XGBoost configuration / GPU加速的XGBoost配置\n",
    "xgb_gpu_config = {\n",
    "    **xgb_config,\n",
    "    \"tree_method\": \"gpu_hist\",  # Use GPU\n",
    "    \"gpu_id\": 0,\n",
    "    \"predictor\": \"gpu_predictor\",\n",
    "}\n",
    "\n",
    "# Create XGBoost models / 创建XGBoost模型\n",
    "xgb_model = XGBModel(**xgb_config)\n",
    "# xgb_gpu_model = XGBModel(**xgb_gpu_config)  # Uncomment if GPU available\n",
    "\n",
    "# Add to pipeline / 添加到管道\n",
    "pipeline.add_model(\"XGBoost\", xgb_model)\n",
    "# pipeline.add_model(\"XGBoost_GPU\", xgb_gpu_model)\n",
    "\n",
    "print(f\"XGBoost model configured with {len(xgb_config)} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CatBoost Models / CatBoost模型 <a id='catboost'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost model configuration / CatBoost模型配置\n",
    "\n",
    "from qlib.contrib.model.catboost import CatBoostModel\n",
    "\n",
    "# CatBoost configuration / CatBoost配置\n",
    "catboost_config = {\n",
    "    \"iterations\": 1000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"depth\": 8,\n",
    "    \"l2_leaf_reg\": 100,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bylevel\": 0.9,\n",
    "    \"random_seed\": 42,\n",
    "    \"loss_function\": \"RMSE\",\n",
    "    \"eval_metric\": \"RMSE\",\n",
    "    \"use_best_model\": True,\n",
    "    \"verbose\": False,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"thread_count\": -1,\n",
    "}\n",
    "\n",
    "# CatBoost with categorical features / 带分类特征的CatBoost\n",
    "catboost_categorical_config = {\n",
    "    **catboost_config,\n",
    "    \"cat_features\": [],  # Specify categorical feature indices\n",
    "    \"one_hot_max_size\": 10,\n",
    "    \"has_time\": True,  # For time series\n",
    "}\n",
    "\n",
    "# Create CatBoost models / 创建CatBoost模型\n",
    "catboost_model = CatBoostModel(**catboost_config)\n",
    "\n",
    "# Add to pipeline / 添加到管道\n",
    "pipeline.add_model(\"CatBoost\", catboost_model)\n",
    "\n",
    "print(f\"CatBoost model configured with {len(catboost_config)} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tree Model Comparison / 树模型对比 <a id='tree-comparison'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tree-based models / 比较树模型\n",
    "\n",
    "def compare_tree_models(models_dict, dataset, test_size=1000):\n",
    "    \"\"\"Compare different tree-based models\n",
    "    比较不同的树模型\n",
    "    \"\"\"\n",
    "    comparison_results = []\n",
    "    \n",
    "    # Get a small test sample for quick comparison\n",
    "    test_data = dataset.prepare(\"test\", col_set=['feature', 'label'], data_key=DataHandlerLP.DK_L).head(test_size)\n",
    "    \n",
    "    for name, model_config in models_dict.items():\n",
    "        print(f\"\\nEvaluating {name}...\")\n",
    "        \n",
    "        # Train model\n",
    "        start_time = time.time()\n",
    "        # model.fit(dataset)  # Would run actual training\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Make predictions\n",
    "        start_time = time.time()\n",
    "        # predictions = model.predict(dataset, segment=\"test\")  # Would make actual predictions\n",
    "        predict_time = time.time() - start_time\n",
    "        \n",
    "        # Store results\n",
    "        comparison_results.append({\n",
    "            'Model': name,\n",
    "            'Parameters': len(model_config),\n",
    "            'Training Time': train_time,\n",
    "            'Prediction Time': predict_time,\n",
    "            'Memory Usage': 0,  # Would measure actual memory\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(comparison_results)\n",
    "\n",
    "# Define models for comparison / 定义用于比较的模型\n",
    "tree_models = {\n",
    "    'LightGBM': lgb_basic_config,\n",
    "    'XGBoost': xgb_config,\n",
    "    'CatBoost': catboost_config,\n",
    "}\n",
    "\n",
    "# Compare models / 比较模型\n",
    "comparison_df = compare_tree_models(tree_models, dataset)\n",
    "print(\"\\nTree Model Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Visualize comparison / 可视化比较\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Parameters comparison / 参数比较\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['Parameters'])\n",
    "axes[0].set_title('Model Complexity (Parameter Count)')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Number of Parameters')\n",
    "\n",
    "# Feature importance comparison (placeholder) / 特征重要性比较（占位符）\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': ['f1', 'f2', 'f3', 'f4', 'f5'],\n",
    "    'LightGBM': np.random.random(5),\n",
    "    'XGBoost': np.random.random(5),\n",
    "    'CatBoost': np.random.random(5),\n",
    "})\n",
    "feature_importance.set_index('Feature').plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Top 5 Feature Importance Comparison')\n",
    "axes[1].set_xlabel('Feature')\n",
    "axes[1].set_ylabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Deep Learning Models / 深度学习模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Neural Network Basics / 神经网络基础 <a id='nn-basics'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network base configuration / 神经网络基础配置\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check GPU availability / 检查GPU可用性\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Base neural network configuration / 基础神经网络配置\n",
    "nn_base_config = {\n",
    "    \"input_dim\": 158,  # Alpha158 features\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"output_dim\": 1,\n",
    "    \"dropout\": 0.3,\n",
    "    \"activation\": \"relu\",\n",
    "    \"batch_norm\": True,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 1024,\n",
    "    \"epochs\": 100,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss_fn\": \"mse\",\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "print(\"Neural network base configuration:\")\n",
    "for key, value in nn_base_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. MLP for Stock Prediction / 用于股票预测的MLP <a id='mlp'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model implementation / MLP模型实现\n",
    "\n",
    "from qlib.contrib.model.pytorch_nn import DNNModel\n",
    "\n",
    "# MLP configuration / MLP配置\n",
    "mlp_config = {\n",
    "    \"batch_size\": 1024,\n",
    "    \"max_steps\": 8000,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"eval_steps\": 20,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"mse\",\n",
    "    \"GPU\": 0 if torch.cuda.is_available() else None,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Advanced MLP with custom architecture / 带自定义架构的高级MLP\n",
    "class CustomMLP(nn.Module):\n",
    "    \"\"\"Custom MLP architecture for stock prediction\n",
    "    用于股票预测的自定义MLP架构\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=158, hidden_dims=[512, 256, 128, 64], \n",
    "                 dropout=0.3, use_batch_norm=True):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            \n",
    "            # Batch normalization\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "            # Activation\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            # Dropout\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            \n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Create MLP models / 创建MLP模型\n",
    "mlp_model = DNNModel(**mlp_config)\n",
    "\n",
    "# Add to pipeline / 添加到管道\n",
    "pipeline.add_model(\"MLP\", mlp_model)\n",
    "\n",
    "print(\"✅ MLP model created\")\n",
    "\n",
    "# Display architecture / 显示架构\n",
    "custom_mlp = CustomMLP()\n",
    "print(\"\\nCustom MLP Architecture:\")\n",
    "print(custom_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. LSTM and GRU Models / LSTM和GRU模型 <a id='lstm-gru'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM and GRU models for time series / 用于时间序列的LSTM和GRU模型\n",
    "\n",
    "from qlib.contrib.model.pytorch_lstm import LSTMModel\n",
    "from qlib.contrib.model.pytorch_gru import GRUModel\n",
    "\n",
    "# LSTM configuration / LSTM配置\n",
    "lstm_config = {\n",
    "    \"d_feat\": 158,  # Input dimension\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"batch_size\": 800,\n",
    "    \"early_stop\": 20,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"metric\": \"loss\",\n",
    "    \"loss\": \"mse\",\n",
    "    \"n_epochs\": 100,\n",
    "    \"GPU\": 0 if torch.cuda.is_available() else None,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# GRU configuration / GRU配置\n",
    "gru_config = {\n",
    "    \"d_feat\": 158,\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"batch_size\": 800,\n",
    "    \"early_stop\": 20,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"metric\": \"loss\",\n",
    "    \"loss\": \"mse\",\n",
    "    \"n_epochs\": 100,\n",
    "    \"GPU\": 0 if torch.cuda.is_available() else None,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Custom LSTM with attention mechanism / 带注意力机制的自定义LSTM\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    \"\"\"LSTM with attention mechanism for better feature extraction\n",
    "    带注意力机制的LSTM用于更好的特征提取\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.3):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Attention layers\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Output\n",
    "        output = self.output(context_vector)\n",
    "        return output\n",
    "\n",
    "# Create LSTM and GRU models / 创建LSTM和GRU模型\n",
    "lstm_model = LSTMModel(**lstm_config)\n",
    "gru_model = GRUModel(**gru_config)\n",
    "\n",
    "# Add to pipeline / 添加到管道\n",
    "pipeline.add_model(\"LSTM\", lstm_model)\n",
    "pipeline.add_model(\"GRU\", gru_model)\n",
    "\n",
    "print(\"✅ LSTM and GRU models created\")\n",
    "\n",
    "# Display custom architecture / 显示自定义架构\n",
    "lstm_attention = LSTMWithAttention(158, 128, 2)\n",
    "print(\"\\nLSTM with Attention Architecture:\")\n",
    "print(lstm_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Transformer Models / Transformer模型 <a id='transformer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model for stock prediction / 用于股票预测的Transformer模型\n",
    "\n",
    "class StockTransformer(nn.Module):\n",
    "    \"\"\"Transformer model for stock prediction\n",
    "    用于股票预测的Transformer模型\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=158, d_model=128, nhead=8, \n",
    "                 num_layers=3, dropout=0.1):\n",
    "        super(StockTransformer, self).__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Output layers\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input projection\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Global pooling (take mean across sequence)\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Output\n",
    "        return self.output(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for transformer\n",
    "    Transformer的位置编码\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Create transformer model / 创建Transformer模型\n",
    "transformer_model = StockTransformer()\n",
    "print(\"✅ Transformer model created\")\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in transformer_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model Optimization / 模型优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Hyperparameter Tuning / 超参数调优 <a id='hyperparameter'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with Optuna / 使用Optuna进行超参数调优\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    \"\"\"Automated hyperparameter tuning framework\n",
    "    自动超参数调优框架\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_class, dataset, n_trials=50):\n",
    "        self.model_class = model_class\n",
    "        self.dataset = dataset\n",
    "        self.n_trials = n_trials\n",
    "        self.best_params = None\n",
    "        self.best_score = None\n",
    "    \n",
    "    def objective_lgb(self, trial):\n",
    "        \"\"\"Objective function for LightGBM\n",
    "        LightGBM的目标函数\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 100.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 100.0, log=True),\n",
    "            'loss': 'mse',\n",
    "            'early_stopping_rounds': 50,\n",
    "            'num_threads': 20,\n",
    "        }\n",
    "        \n",
    "        # Train model with suggested parameters\n",
    "        model = LGBModel(**params)\n",
    "        model.fit(self.dataset)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        pred_valid = model.predict(self.dataset, segment=\"valid\")\n",
    "        label_valid = self.dataset.prepare(\"valid\", col_set=['label'], data_key=DataHandlerLP.DK_L)\n",
    "        \n",
    "        # Calculate IC as objective\n",
    "        df = pd.DataFrame({'pred': pred_valid, 'label': label_valid['label']}).dropna()\n",
    "        ic = df['pred'].corr(df['label'])\n",
    "        \n",
    "        return ic  # Optuna maximizes by default\n",
    "    \n",
    "    def objective_nn(self, trial):\n",
    "        \"\"\"Objective function for neural networks\n",
    "        神经网络的目标函数\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'hidden_size': trial.suggest_categorical('hidden_size', [64, 128, 256, 512]),\n",
    "            'num_layers': trial.suggest_int('num_layers', 1, 4),\n",
    "            'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [256, 512, 1024, 2048]),\n",
    "            'weight_decay': trial.suggest_float('weight_decay', 1e-8, 1e-3, log=True),\n",
    "        }\n",
    "        \n",
    "        # Train and evaluate model\n",
    "        # ... (implementation)\n",
    "        \n",
    "        return 0  # Placeholder\n",
    "    \n",
    "    def tune(self, model_type='lgb'):\n",
    "        \"\"\"Run hyperparameter tuning\n",
    "        运行超参数调优\n",
    "        \"\"\"\n",
    "        print(f\"Starting hyperparameter tuning for {model_type}...\")\n",
    "        \n",
    "        # Select objective function\n",
    "        if model_type == 'lgb':\n",
    "            objective = self.objective_lgb\n",
    "        elif model_type == 'nn':\n",
    "            objective = self.objective_nn\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        # Create study\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=TPESampler(seed=42)\n",
    "        )\n",
    "        \n",
    "        # Optimize\n",
    "        study.optimize(objective, n_trials=self.n_trials)\n",
    "        \n",
    "        # Store best parameters\n",
    "        self.best_params = study.best_params\n",
    "        self.best_score = study.best_value\n",
    "        \n",
    "        print(f\"\\nBest score: {self.best_score:.4f}\")\n",
    "        print(f\"Best parameters: {self.best_params}\")\n",
    "        \n",
    "        return study\n",
    "\n",
    "# Example tuning (would run actual tuning)\n",
    "print(\"Hyperparameter tuning framework ready\")\n",
    "print(\"\\nExample parameter search space for LightGBM:\")\n",
    "print(\"  num_leaves: [20, 300]\")\n",
    "print(\"  learning_rate: [0.01, 0.3]\")\n",
    "print(\"  max_depth: [3, 12]\")\n",
    "print(\"  subsample: [0.5, 1.0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Feature Selection / 特征选择 <a id='feature-selection'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection methods / 特征选择方法\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class FeatureSelector:\n",
    "    \"\"\"Comprehensive feature selection framework\n",
    "    综合特征选择框架\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.selected_features = {}\n",
    "        self.feature_scores = {}\n",
    "    \n",
    "    def select_by_importance(self, model, top_k=50):\n",
    "        \"\"\"Select features by model importance\n",
    "        通过模型重要性选择特征\n",
    "        \"\"\"\n",
    "        # Train model\n",
    "        model.fit(self.dataset)\n",
    "        \n",
    "        # Get feature importance\n",
    "        if hasattr(model, 'feature_importance'):\n",
    "            importance = model.feature_importance()\n",
    "            \n",
    "            # Sort and select top features\n",
    "            sorted_idx = np.argsort(importance)[::-1]\n",
    "            selected_idx = sorted_idx[:top_k]\n",
    "            \n",
    "            # Get feature names\n",
    "            train_data = self.dataset.prepare(\"train\", col_set=['feature'])\n",
    "            feature_names = train_data.columns.tolist()\n",
    "            \n",
    "            self.selected_features['importance'] = [feature_names[i] for i in selected_idx]\n",
    "            self.feature_scores['importance'] = importance[selected_idx]\n",
    "            \n",
    "            return self.selected_features['importance']\n",
    "    \n",
    "    def select_by_mutual_info(self, top_k=50):\n",
    "        \"\"\"Select features by mutual information\n",
    "        通过互信息选择特征\n",
    "        \"\"\"\n",
    "        # Get data\n",
    "        train_data = self.dataset.prepare(\"train\", col_set=['feature', 'label'])\n",
    "        X = train_data.iloc[:, :-1]\n",
    "        y = train_data.iloc[:, -1]\n",
    "        \n",
    "        # Calculate mutual information\n",
    "        selector = SelectKBest(mutual_info_regression, k=top_k)\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get selected features\n",
    "        selected_mask = selector.get_support()\n",
    "        self.selected_features['mutual_info'] = X.columns[selected_mask].tolist()\n",
    "        self.feature_scores['mutual_info'] = selector.scores_[selected_mask]\n",
    "        \n",
    "        return self.selected_features['mutual_info']\n",
    "    \n",
    "    def select_by_correlation(self, threshold=0.95):\n",
    "        \"\"\"Remove highly correlated features\n",
    "        移除高度相关的特征\n",
    "        \"\"\"\n",
    "        # Get data\n",
    "        train_data = self.dataset.prepare(\"train\", col_set=['feature'])\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = train_data.corr().abs()\n",
    "        \n",
    "        # Find features to remove\n",
    "        upper_tri = corr_matrix.where(\n",
    "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "        )\n",
    "        \n",
    "        to_drop = [column for column in upper_tri.columns \n",
    "                  if any(upper_tri[column] > threshold)]\n",
    "        \n",
    "        # Keep features\n",
    "        self.selected_features['correlation'] = [\n",
    "            col for col in train_data.columns if col not in to_drop\n",
    "        ]\n",
    "        \n",
    "        return self.selected_features['correlation']\n",
    "    \n",
    "    def select_by_recursive_elimination(self, estimator=None, n_features=50):\n",
    "        \"\"\"Recursive feature elimination\n",
    "        递归特征消除\n",
    "        \"\"\"\n",
    "        from sklearn.feature_selection import RFE\n",
    "        \n",
    "        # Get data\n",
    "        train_data = self.dataset.prepare(\"train\", col_set=['feature', 'label'])\n",
    "        X = train_data.iloc[:, :-1]\n",
    "        y = train_data.iloc[:, -1]\n",
    "        \n",
    "        # Use default estimator if not provided\n",
    "        if estimator is None:\n",
    "            estimator = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        \n",
    "        # RFE\n",
    "        selector = RFE(estimator, n_features_to_select=n_features)\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get selected features\n",
    "        self.selected_features['rfe'] = X.columns[selector.support_].tolist()\n",
    "        \n",
    "        return self.selected_features['rfe']\n",
    "    \n",
    "    def combine_selections(self, methods=['importance', 'mutual_info'], min_votes=2):\n",
    "        \"\"\"Combine multiple feature selection methods\n",
    "        组合多种特征选择方法\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        \n",
    "        # Count votes for each feature\n",
    "        all_features = []\n",
    "        for method in methods:\n",
    "            if method in self.selected_features:\n",
    "                all_features.extend(self.selected_features[method])\n",
    "        \n",
    "        feature_votes = Counter(all_features)\n",
    "        \n",
    "        # Select features with enough votes\n",
    "        combined_features = [\n",
    "            feature for feature, votes in feature_votes.items() \n",
    "            if votes >= min_votes\n",
    "        ]\n",
    "        \n",
    "        return combined_features\n",
    "\n",
    "# Example usage\n",
    "print(\"Feature selection framework ready\")\n",
    "print(\"\\nAvailable methods:\")\n",
    "print(\"  1. Model importance-based selection\")\n",
    "print(\"  2. Mutual information selection\")\n",
    "print(\"  3. Correlation-based removal\")\n",
    "print(\"  4. Recursive feature elimination\")\n",
    "print(\"  5. Ensemble voting combination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Ensemble Methods / 集成方法 <a id='ensemble'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble methods for model combination / 模型组合的集成方法\n",
    "\n",
    "class EnsembleModel:\n",
    "    \"\"\"Ensemble model framework\n",
    "    集成模型框架\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models, weights=None, method='average'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - models: List of trained models\n",
    "        - weights: Model weights for weighted average\n",
    "        - method: 'average', 'weighted', 'stacking', 'blending'\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.weights = weights or [1/len(models)] * len(models)\n",
    "        self.method = method\n",
    "        self.meta_model = None\n",
    "    \n",
    "    def predict_average(self, dataset, segment=\"test\"):\n",
    "        \"\"\"Simple average ensemble\n",
    "        简单平均集成\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            pred = model.predict(dataset, segment)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Stack predictions and take mean\n",
    "        pred_matrix = np.column_stack(predictions)\n",
    "        ensemble_pred = pred_matrix.mean(axis=1)\n",
    "        \n",
    "        return ensemble_pred\n",
    "    \n",
    "    def predict_weighted(self, dataset, segment=\"test\"):\n",
    "        \"\"\"Weighted average ensemble\n",
    "        加权平均集成\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for model, weight in zip(self.models, self.weights):\n",
    "            pred = model.predict(dataset, segment)\n",
    "            predictions.append(pred * weight)\n",
    "        \n",
    "        # Sum weighted predictions\n",
    "        ensemble_pred = np.sum(predictions, axis=0)\n",
    "        \n",
    "        return ensemble_pred\n",
    "    \n",
    "    def train_stacking(self, dataset):\n",
    "        \"\"\"Train stacking ensemble\n",
    "        训练堆叠集成\n",
    "        \"\"\"\n",
    "        # Get base model predictions on validation set\n",
    "        valid_predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            pred = model.predict(dataset, segment=\"valid\")\n",
    "            valid_predictions.append(pred)\n",
    "        \n",
    "        # Stack predictions as features\n",
    "        X_meta = np.column_stack(valid_predictions)\n",
    "        \n",
    "        # Get validation labels\n",
    "        y_valid = dataset.prepare(\"valid\", col_set=['label'])['label'].values\n",
    "        \n",
    "        # Train meta-model\n",
    "        from sklearn.linear_model import Ridge\n",
    "        self.meta_model = Ridge(alpha=1.0)\n",
    "        self.meta_model.fit(X_meta, y_valid)\n",
    "        \n",
    "        print(\"Stacking meta-model trained\")\n",
    "    \n",
    "    def predict_stacking(self, dataset, segment=\"test\"):\n",
    "        \"\"\"Predict with stacking ensemble\n",
    "        使用堆叠集成预测\n",
    "        \"\"\"\n",
    "        if self.meta_model is None:\n",
    "            raise ValueError(\"Must train stacking model first\")\n",
    "        \n",
    "        # Get base model predictions\n",
    "        test_predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            pred = model.predict(dataset, segment)\n",
    "            test_predictions.append(pred)\n",
    "        \n",
    "        # Stack predictions\n",
    "        X_meta = np.column_stack(test_predictions)\n",
    "        \n",
    "        # Meta-model prediction\n",
    "        ensemble_pred = self.meta_model.predict(X_meta)\n",
    "        \n",
    "        return ensemble_pred\n",
    "    \n",
    "    def optimize_weights(self, dataset):\n",
    "        \"\"\"Optimize ensemble weights\n",
    "        优化集成权重\n",
    "        \"\"\"\n",
    "        from scipy.optimize import minimize\n",
    "        \n",
    "        # Get validation predictions\n",
    "        valid_predictions = []\n",
    "        for model in self.models:\n",
    "            pred = model.predict(dataset, segment=\"valid\")\n",
    "            valid_predictions.append(pred)\n",
    "        \n",
    "        pred_matrix = np.column_stack(valid_predictions)\n",
    "        \n",
    "        # Get validation labels\n",
    "        y_valid = dataset.prepare(\"valid\", col_set=['label'])['label'].values\n",
    "        \n",
    "        # Objective function\n",
    "        def objective(weights):\n",
    "            weighted_pred = np.dot(pred_matrix, weights)\n",
    "            mse = np.mean((weighted_pred - y_valid) ** 2)\n",
    "            return mse\n",
    "        \n",
    "        # Constraints: weights sum to 1, all positive\n",
    "        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "        bounds = [(0, 1)] * len(self.models)\n",
    "        \n",
    "        # Initial weights\n",
    "        init_weights = [1/len(self.models)] * len(self.models)\n",
    "        \n",
    "        # Optimize\n",
    "        result = minimize(objective, init_weights, method='SLSQP',\n",
    "                        bounds=bounds, constraints=constraints)\n",
    "        \n",
    "        self.weights = result.x\n",
    "        print(f\"Optimized weights: {self.weights}\")\n",
    "        \n",
    "        return self.weights\n",
    "\n",
    "print(\"Ensemble framework ready\")\n",
    "print(\"\\nAvailable ensemble methods:\")\n",
    "print(\"  1. Simple averaging\")\n",
    "print(\"  2. Weighted averaging\")\n",
    "print(\"  3. Stacking with meta-learner\")\n",
    "print(\"  4. Weight optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Production Deployment / 生产部署"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Model Persistence / 模型持久化 <a id='persistence'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model persistence and versioning / 模型持久化和版本控制\n",
    "\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Model management system for production\n",
    "    生产环境的模型管理系统\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir=\"./models\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "        self.model_registry = {}\n",
    "    \n",
    "    def save_model(self, model, name, version=None, metadata=None):\n",
    "        \"\"\"Save model with versioning\n",
    "        带版本控制的模型保存\n",
    "        \"\"\"\n",
    "        # Generate version if not provided\n",
    "        if version is None:\n",
    "            version = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Create model directory\n",
    "        model_dir = self.base_dir / name / version\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        model_path = model_dir / \"model.pkl\"\n",
    "        joblib.dump(model, model_path)\n",
    "        \n",
    "        # Save metadata\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        \n",
    "        metadata.update({\n",
    "            'name': name,\n",
    "            'version': version,\n",
    "            'saved_at': datetime.now().isoformat(),\n",
    "            'model_class': model.__class__.__name__,\n",
    "            'file_hash': self._calculate_hash(model_path)\n",
    "        })\n",
    "        \n",
    "        metadata_path = model_dir / \"metadata.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        # Update registry\n",
    "        self.model_registry[f\"{name}:{version}\"] = {\n",
    "            'path': model_path,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        \n",
    "        print(f\"Model saved: {name}:{version}\")\n",
    "        return model_path\n",
    "    \n",
    "    def load_model(self, name, version=\"latest\"):\n",
    "        \"\"\"Load model by name and version\n",
    "        按名称和版本加载模型\n",
    "        \"\"\"\n",
    "        # Find model version\n",
    "        if version == \"latest\":\n",
    "            model_versions = list((self.base_dir / name).glob(\"*\"))\n",
    "            if not model_versions:\n",
    "                raise ValueError(f\"No versions found for model {name}\")\n",
    "            version = sorted(model_versions)[-1].name\n",
    "        \n",
    "        # Load model\n",
    "        model_path = self.base_dir / name / version / \"model.pkl\"\n",
    "        if not model_path.exists():\n",
    "            raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "        \n",
    "        model = joblib.load(model_path)\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = self.base_dir / name / version / \"metadata.json\"\n",
    "        if metadata_path.exists():\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "        else:\n",
    "            metadata = {}\n",
    "        \n",
    "        print(f\"Model loaded: {name}:{version}\")\n",
    "        return model, metadata\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List all available models\n",
    "        列出所有可用模型\n",
    "        \"\"\"\n",
    "        models = []\n",
    "        \n",
    "        for model_dir in self.base_dir.glob(\"*\"):\n",
    "            if model_dir.is_dir():\n",
    "                for version_dir in model_dir.glob(\"*\"):\n",
    "                    if version_dir.is_dir():\n",
    "                        models.append({\n",
    "                            'name': model_dir.name,\n",
    "                            'version': version_dir.name,\n",
    "                            'path': version_dir\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(models)\n",
    "    \n",
    "    def _calculate_hash(self, file_path):\n",
    "        \"\"\"Calculate file hash for integrity check\n",
    "        计算文件哈希值以进行完整性检查\n",
    "        \"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "\n",
    "# Example usage\n",
    "model_manager = ModelManager()\n",
    "print(\"Model manager initialized\")\n",
    "print(f\"Model directory: {model_manager.base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Model Monitoring / 模型监控 <a id='monitoring'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model monitoring system / 模型监控系统\n",
    "\n",
    "class ModelMonitor:\n",
    "    \"\"\"Production model monitoring\n",
    "    生产模型监控\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.metrics_history = []\n",
    "        self.alerts = []\n",
    "        self.thresholds = {\n",
    "            'ic_min': 0.01,\n",
    "            'prediction_std_max': 0.1,\n",
    "            'null_rate_max': 0.05\n",
    "        }\n",
    "    \n",
    "    def monitor_prediction_quality(self, predictions, labels=None):\n",
    "        \"\"\"Monitor prediction quality metrics\n",
    "        监控预测质量指标\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Basic statistics\n",
    "        metrics['mean'] = np.mean(predictions)\n",
    "        metrics['std'] = np.std(predictions)\n",
    "        metrics['min'] = np.min(predictions)\n",
    "        metrics['max'] = np.max(predictions)\n",
    "        metrics['null_rate'] = np.isnan(predictions).mean()\n",
    "        \n",
    "        # IC if labels available\n",
    "        if labels is not None:\n",
    "            valid_idx = ~(np.isnan(predictions) | np.isnan(labels))\n",
    "            if valid_idx.sum() > 0:\n",
    "                metrics['ic'] = np.corrcoef(\n",
    "                    predictions[valid_idx], \n",
    "                    labels[valid_idx]\n",
    "                )[0, 1]\n",
    "        \n",
    "        # Check alerts\n",
    "        self._check_alerts(metrics)\n",
    "        \n",
    "        # Store metrics\n",
    "        metrics['timestamp'] = datetime.now()\n",
    "        self.metrics_history.append(metrics)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _check_alerts(self, metrics):\n",
    "        \"\"\"Check if any alerts should be triggered\n",
    "        检查是否应触发任何警报\n",
    "        \"\"\"\n",
    "        # IC degradation\n",
    "        if 'ic' in metrics and metrics['ic'] < self.thresholds['ic_min']:\n",
    "            self.alerts.append({\n",
    "                'type': 'IC_DEGRADATION',\n",
    "                'message': f\"IC {metrics['ic']:.4f} below threshold {self.thresholds['ic_min']}\",\n",
    "                'timestamp': datetime.now()\n",
    "            })\n",
    "        \n",
    "        # High null rate\n",
    "        if metrics['null_rate'] > self.thresholds['null_rate_max']:\n",
    "            self.alerts.append({\n",
    "                'type': 'HIGH_NULL_RATE',\n",
    "                'message': f\"Null rate {metrics['null_rate']:.2%} exceeds threshold\",\n",
    "                'timestamp': datetime.now()\n",
    "            })\n",
    "    \n",
    "    def get_monitoring_report(self):\n",
    "        \"\"\"Generate monitoring report\n",
    "        生成监控报告\n",
    "        \"\"\"\n",
    "        if not self.metrics_history:\n",
    "            return \"No metrics available\"\n",
    "        \n",
    "        report = f\"\\nModel Monitoring Report: {self.model_name}\\n\"\n",
    "        report += \"=\"*50 + \"\\n\"\n",
    "        \n",
    "        # Recent metrics\n",
    "        recent = self.metrics_history[-1]\n",
    "        report += \"\\nRecent Metrics:\\n\"\n",
    "        for key, value in recent.items():\n",
    "            if key != 'timestamp':\n",
    "                report += f\"  {key}: {value:.4f}\\n\"\n",
    "        \n",
    "        # Alerts\n",
    "        if self.alerts:\n",
    "            report += f\"\\nAlerts ({len(self.alerts)}):\\n\"\n",
    "            for alert in self.alerts[-5:]:  # Last 5 alerts\n",
    "                report += f\"  [{alert['type']}] {alert['message']}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Example monitoring\n",
    "monitor = ModelMonitor(\"LightGBM_Production\")\n",
    "\n",
    "# Simulate monitoring\n",
    "for i in range(5):\n",
    "    predictions = np.random.randn(1000) * 0.01\n",
    "    labels = np.random.randn(1000) * 0.01\n",
    "    metrics = monitor.monitor_prediction_quality(predictions, labels)\n",
    "\n",
    "print(monitor.get_monitoring_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary / 总结\n",
    "\n",
    "### What we covered / 本章内容\n",
    "\n",
    "#### Part 1: Model Fundamentals / 模型基础\n",
    "- Model interface and architecture / 模型接口与架构\n",
    "- Dataset preparation / 数据集准备\n",
    "- Training pipeline / 训练管道\n",
    "\n",
    "#### Part 2: Tree-Based Models / 树模型\n",
    "- **LightGBM**: Fast and efficient gradient boosting / 快速高效的梯度提升\n",
    "- **XGBoost**: Scalable gradient boosting / 可扩展的梯度提升\n",
    "- **CatBoost**: Handling categorical features / 处理分类特征\n",
    "\n",
    "#### Part 3: Deep Learning Models / 深度学习模型\n",
    "- **MLP**: Multi-layer perceptrons / 多层感知器\n",
    "- **LSTM/GRU**: Sequential models / 序列模型\n",
    "- **Transformer**: Attention-based models / 基于注意力的模型\n",
    "- **GNN**: Graph neural networks / 图神经网络\n",
    "\n",
    "#### Part 4: Model Optimization / 模型优化\n",
    "- **Hyperparameter Tuning**: Optuna optimization / Optuna优化\n",
    "- **Feature Selection**: Multiple methods / 多种方法\n",
    "- **Ensemble Methods**: Model combination / 模型组合\n",
    "- **Cross-Validation**: Robust evaluation / 稳健评估\n",
    "\n",
    "#### Part 5: Production Deployment / 生产部署\n",
    "- **Model Persistence**: Versioning and storage / 版本控制和存储\n",
    "- **Online Learning**: Incremental updates / 增量更新\n",
    "- **Model Monitoring**: Performance tracking / 性能跟踪\n",
    "- **Best Practices**: Production guidelines / 生产指南\n",
    "\n",
    "### Key Takeaways / 关键要点\n",
    "\n",
    "1. **Model Selection**: Choose models based on data characteristics / 根据数据特征选择模型\n",
    "2. **Feature Engineering**: Critical for performance / 对性能至关重要\n",
    "3. **Hyperparameter Tuning**: Systematic optimization / 系统化优化\n",
    "4. **Ensemble Methods**: Often improve performance / 通常能提高性能\n",
    "5. **Production Readiness**: Monitor and maintain models / 监控和维护模型\n",
    "\n",
    "### Next Steps / 下一步\n",
    "\n",
    "Continue with **[04_evaluation_module.ipynb](./04_evaluation_module.ipynb)** to learn about:\n",
    "- Backtesting strategies / 回测策略\n",
    "- Performance evaluation / 性能评估\n",
    "- Risk analysis / 风险分析"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}