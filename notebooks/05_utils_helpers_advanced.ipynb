{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Utilities, Helpers and Advanced Features\n",
    "# ç¬¬äº”ç« ï¼šå·¥å…·å‡½æ•°ã€è¾…åŠ©åŠŸèƒ½ä¸é«˜çº§ç‰¹æ€§\n",
    "\n",
    "**Author**: Microsoft Qlib Team  \n",
    "**License**: MIT License  \n",
    "**Last Updated**: 2025-01-09\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Table of Contents / ç›®å½•\n",
    "\n",
    "### Part 1: Core Utilities / æ ¸å¿ƒå·¥å…·\n",
    "1. [Workflow Automation / å·¥ä½œæµè‡ªåŠ¨åŒ–](#workflow-automation)\n",
    "2. [Experiment Management / å®éªŒç®¡ç†](#experiment-management)\n",
    "3. [Data Utilities / æ•°æ®å·¥å…·](#data-utilities)\n",
    "4. [Feature Engineering Helpers / ç‰¹å¾å·¥ç¨‹è¾…åŠ©](#feature-engineering)\n",
    "\n",
    "### Part 2: Advanced Features / é«˜çº§åŠŸèƒ½\n",
    "5. [Multi-Market Support / å¤šå¸‚åœºæ”¯æŒ](#multi-market)\n",
    "6. [Online Learning / åœ¨çº¿å­¦ä¹ ](#online-learning)\n",
    "7. [Meta-Learning / å…ƒå­¦ä¹ ](#meta-learning)\n",
    "8. [AutoML Integration / è‡ªåŠ¨æœºå™¨å­¦ä¹ é›†æˆ](#automl)\n",
    "\n",
    "### Part 3: Production Tools / ç”Ÿäº§å·¥å…·\n",
    "9. [Real-time Trading / å®æ—¶äº¤æ˜“](#realtime-trading)\n",
    "10. [Monitoring and Alerting / ç›‘æ§ä¸å‘Šè­¦](#monitoring)\n",
    "11. [Debugging and Profiling / è°ƒè¯•ä¸æ€§èƒ½åˆ†æ](#debugging)\n",
    "12. [Deployment Tools / éƒ¨ç½²å·¥å…·](#deployment)\n",
    "\n",
    "### Part 4: Best Practices / æœ€ä½³å®è·µ\n",
    "13. [Code Organization / ä»£ç ç»„ç»‡](#code-organization)\n",
    "14. [Performance Optimization / æ€§èƒ½ä¼˜åŒ–](#performance-optimization)\n",
    "15. [Common Pitfalls and Solutions / å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#pitfalls)\n",
    "16. [Tips and Tricks / æŠ€å·§ä¸çªé—¨](#tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports / è®¾ç½®å’Œå¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports / å¿…è¦å¯¼å…¥\n",
    "import qlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "import yaml\n",
    "import logging\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "import concurrent.futures\n",
    "from functools import wraps, lru_cache\n",
    "import traceback\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Qlib imports / Qlibå¯¼å…¥\n",
    "from qlib.data import D\n",
    "from qlib.config import C\n",
    "from qlib.workflow import R\n",
    "from qlib.utils import init_instance_by_config, flatten_dict\n",
    "from qlib.log import get_module_logger\n",
    "\n",
    "# Initialize Qlib / åˆå§‹åŒ–Qlib\n",
    "qlib.init()\n",
    "\n",
    "# Setup logging / è®¾ç½®æ—¥å¿—\n",
    "logger = get_module_logger(\"UtilsHelpers\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "print(\"âœ… Environment initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Core Utilities / æ ¸å¿ƒå·¥å…·"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Workflow Automation / å·¥ä½œæµè‡ªåŠ¨åŒ– <a id='workflow-automation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow automation utilities / å·¥ä½œæµè‡ªåŠ¨åŒ–å·¥å…·\n",
    "\n",
    "class WorkflowAutomation:\n",
    "    \"\"\"Complete workflow automation framework\n",
    "    å®Œæ•´çš„å·¥ä½œæµè‡ªåŠ¨åŒ–æ¡†æ¶\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str = None):\n",
    "        self.config = self.load_config(config_path) if config_path else {}\n",
    "        self.pipeline_steps = []\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_config(self, config_path: str) -> dict:\n",
    "        \"\"\"Load workflow configuration / åŠ è½½å·¥ä½œæµé…ç½®\"\"\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            if config_path.endswith('.yaml'):\n",
    "                return yaml.safe_load(f)\n",
    "            elif config_path.endswith('.json'):\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def add_step(self, name: str, func, **kwargs):\n",
    "        \"\"\"Add a step to the pipeline / æ·»åŠ ç®¡é“æ­¥éª¤\"\"\"\n",
    "        self.pipeline_steps.append({\n",
    "            'name': name,\n",
    "            'func': func,\n",
    "            'kwargs': kwargs\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def run(self, parallel: bool = False):\n",
    "        \"\"\"Execute the workflow / æ‰§è¡Œå·¥ä½œæµ\"\"\"\n",
    "        print(f\"Starting workflow with {len(self.pipeline_steps)} steps...\")\n",
    "        \n",
    "        if parallel:\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                futures = []\n",
    "                for step in self.pipeline_steps:\n",
    "                    future = executor.submit(self._execute_step, step)\n",
    "                    futures.append((step['name'], future))\n",
    "                \n",
    "                for name, future in futures:\n",
    "                    self.results[name] = future.result()\n",
    "        else:\n",
    "            for step in self.pipeline_steps:\n",
    "                self.results[step['name']] = self._execute_step(step)\n",
    "        \n",
    "        print(\"âœ… Workflow completed\")\n",
    "        return self.results\n",
    "    \n",
    "    def _execute_step(self, step: dict):\n",
    "        \"\"\"Execute a single step / æ‰§è¡Œå•ä¸ªæ­¥éª¤\"\"\"\n",
    "        print(f\"  Executing: {step['name']}...\")\n",
    "        try:\n",
    "            result = step['func'](**step['kwargs'])\n",
    "            print(f\"    âœ… {step['name']} completed\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"    âŒ {step['name']} failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Example workflow / ç¤ºä¾‹å·¥ä½œæµ\n",
    "def data_preparation(market='csi300'):\n",
    "    \"\"\"Prepare data / å‡†å¤‡æ•°æ®\"\"\"\n",
    "    return f\"Data prepared for {market}\"\n",
    "\n",
    "def model_training(data=None):\n",
    "    \"\"\"Train model / è®­ç»ƒæ¨¡å‹\"\"\"\n",
    "    return \"Model trained\"\n",
    "\n",
    "def backtesting(model=None):\n",
    "    \"\"\"Run backtest / è¿è¡Œå›æµ‹\"\"\"\n",
    "    return \"Backtest completed\"\n",
    "\n",
    "# Create and run workflow / åˆ›å»ºå¹¶è¿è¡Œå·¥ä½œæµ\n",
    "workflow = WorkflowAutomation()\n",
    "workflow.add_step('data_prep', data_preparation, market='csi300')\n",
    "workflow.add_step('training', model_training)\n",
    "workflow.add_step('backtest', backtesting)\n",
    "\n",
    "results = workflow.run(parallel=False)\n",
    "print(f\"\\nResults: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline decorator for automatic workflow creation / ç”¨äºè‡ªåŠ¨åˆ›å»ºå·¥ä½œæµçš„ç®¡é“è£…é¥°å™¨\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"Pipeline decorator for workflow automation\n",
    "    ç”¨äºå·¥ä½œæµè‡ªåŠ¨åŒ–çš„ç®¡é“è£…é¥°å™¨\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.steps = []\n",
    "        self.cache = {}\n",
    "    \n",
    "    def step(self, name: str, cache: bool = False):\n",
    "        \"\"\"Decorator to mark a function as a pipeline step\n",
    "        å°†å‡½æ•°æ ‡è®°ä¸ºç®¡é“æ­¥éª¤çš„è£…é¥°å™¨\n",
    "        \"\"\"\n",
    "        def decorator(func):\n",
    "            @wraps(func)\n",
    "            def wrapper(*args, **kwargs):\n",
    "                # Check cache / æ£€æŸ¥ç¼“å­˜\n",
    "                if cache and name in self.cache:\n",
    "                    print(f\"  Using cached result for {name}\")\n",
    "                    return self.cache[name]\n",
    "                \n",
    "                # Execute function / æ‰§è¡Œå‡½æ•°\n",
    "                print(f\"  Executing {name}...\")\n",
    "                start_time = time.time()\n",
    "                result = func(*args, **kwargs)\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                # Cache result if requested / å¦‚æœéœ€è¦åˆ™ç¼“å­˜ç»“æœ\n",
    "                if cache:\n",
    "                    self.cache[name] = result\n",
    "                \n",
    "                print(f\"    âœ… {name} completed in {elapsed:.2f}s\")\n",
    "                return result\n",
    "            \n",
    "            # Register step / æ³¨å†Œæ­¥éª¤\n",
    "            self.steps.append((name, wrapper))\n",
    "            return wrapper\n",
    "        return decorator\n",
    "    \n",
    "    def run_all(self, context: dict = None):\n",
    "        \"\"\"Run all registered steps / è¿è¡Œæ‰€æœ‰æ³¨å†Œçš„æ­¥éª¤\"\"\"\n",
    "        context = context or {}\n",
    "        print(f\"Running pipeline with {len(self.steps)} steps...\")\n",
    "        \n",
    "        for name, func in self.steps:\n",
    "            try:\n",
    "                context[name] = func(context)\n",
    "            except Exception as e:\n",
    "                print(f\"    âŒ Error in {name}: {str(e)}\")\n",
    "                context[name] = None\n",
    "        \n",
    "        return context\n",
    "\n",
    "# Example usage / ä½¿ç”¨ç¤ºä¾‹\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.step(\"load_data\", cache=True)\n",
    "def load_data(context):\n",
    "    \"\"\"Load market data / åŠ è½½å¸‚åœºæ•°æ®\"\"\"\n",
    "    # Simulate data loading\n",
    "    return pd.DataFrame(np.random.randn(100, 5), columns=['A', 'B', 'C', 'D', 'E'])\n",
    "\n",
    "@pipeline.step(\"preprocess\", cache=False)\n",
    "def preprocess(context):\n",
    "    \"\"\"Preprocess data / é¢„å¤„ç†æ•°æ®\"\"\"\n",
    "    data = context.get('load_data')\n",
    "    if data is not None:\n",
    "        return data.fillna(0)\n",
    "    return None\n",
    "\n",
    "@pipeline.step(\"analyze\")\n",
    "def analyze(context):\n",
    "    \"\"\"Analyze data / åˆ†ææ•°æ®\"\"\"\n",
    "    data = context.get('preprocess')\n",
    "    if data is not None:\n",
    "        return data.describe()\n",
    "    return None\n",
    "\n",
    "# Run pipeline / è¿è¡Œç®¡é“\n",
    "results = pipeline.run_all()\n",
    "print(f\"\\nPipeline completed with {len(results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment Management / å®éªŒç®¡ç† <a id='experiment-management'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced experiment management / é«˜çº§å®éªŒç®¡ç†\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Experiment configuration / å®éªŒé…ç½®\"\"\"\n",
    "    name: str\n",
    "    model_type: str\n",
    "    dataset: str\n",
    "    hyperparameters: dict\n",
    "    metrics: List[str]\n",
    "    tags: List[str] = None\n",
    "    description: str = \"\"\n",
    "\n",
    "class ExperimentManager:\n",
    "    \"\"\"Comprehensive experiment management system\n",
    "    ç»¼åˆå®éªŒç®¡ç†ç³»ç»Ÿ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: str = \"./experiments\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "        self.experiments = {}\n",
    "        self.active_experiment = None\n",
    "        \n",
    "    def create_experiment(self, config: ExperimentConfig) -> str:\n",
    "        \"\"\"Create a new experiment / åˆ›å»ºæ–°å®éªŒ\"\"\"\n",
    "        exp_id = f\"{config.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        exp_dir = self.base_dir / exp_id\n",
    "        exp_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save configuration / ä¿å­˜é…ç½®\n",
    "        config_path = exp_dir / \"config.json\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config.__dict__, f, indent=2)\n",
    "        \n",
    "        # Initialize experiment tracking / åˆå§‹åŒ–å®éªŒè·Ÿè¸ª\n",
    "        self.experiments[exp_id] = {\n",
    "            'config': config,\n",
    "            'dir': exp_dir,\n",
    "            'status': 'created',\n",
    "            'created_at': datetime.now(),\n",
    "            'metrics': {},\n",
    "            'artifacts': []\n",
    "        }\n",
    "        \n",
    "        self.active_experiment = exp_id\n",
    "        print(f\"âœ… Created experiment: {exp_id}\")\n",
    "        return exp_id\n",
    "    \n",
    "    def log_metric(self, name: str, value: float, step: int = None):\n",
    "        \"\"\"Log a metric / è®°å½•æŒ‡æ ‡\"\"\"\n",
    "        if self.active_experiment is None:\n",
    "            raise ValueError(\"No active experiment\")\n",
    "        \n",
    "        exp = self.experiments[self.active_experiment]\n",
    "        if name not in exp['metrics']:\n",
    "            exp['metrics'][name] = []\n",
    "        \n",
    "        exp['metrics'][name].append({\n",
    "            'value': value,\n",
    "            'step': step or len(exp['metrics'][name]),\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "    \n",
    "    def log_artifact(self, artifact_path: str, artifact_type: str = 'file'):\n",
    "        \"\"\"Log an artifact / è®°å½•å·¥ä»¶\"\"\"\n",
    "        if self.active_experiment is None:\n",
    "            raise ValueError(\"No active experiment\")\n",
    "        \n",
    "        exp = self.experiments[self.active_experiment]\n",
    "        exp['artifacts'].append({\n",
    "            'path': artifact_path,\n",
    "            'type': artifact_type,\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "    \n",
    "    def compare_experiments(self, exp_ids: List[str] = None):\n",
    "        \"\"\"Compare multiple experiments / æ¯”è¾ƒå¤šä¸ªå®éªŒ\"\"\"\n",
    "        if exp_ids is None:\n",
    "            exp_ids = list(self.experiments.keys())\n",
    "        \n",
    "        comparison = pd.DataFrame()\n",
    "        \n",
    "        for exp_id in exp_ids:\n",
    "            if exp_id in self.experiments:\n",
    "                exp = self.experiments[exp_id]\n",
    "                row = {\n",
    "                    'experiment': exp_id,\n",
    "                    'model': exp['config'].model_type,\n",
    "                    'dataset': exp['config'].dataset,\n",
    "                    'status': exp['status']\n",
    "                }\n",
    "                \n",
    "                # Add final metrics / æ·»åŠ æœ€ç»ˆæŒ‡æ ‡\n",
    "                for metric_name, values in exp['metrics'].items():\n",
    "                    if values:\n",
    "                        row[f'{metric_name}_final'] = values[-1]['value']\n",
    "                \n",
    "                comparison = pd.concat([comparison, pd.DataFrame([row])], ignore_index=True)\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def get_best_experiment(self, metric: str, mode: str = 'max'):\n",
    "        \"\"\"Get the best experiment based on a metric\n",
    "        åŸºäºæŒ‡æ ‡è·å–æœ€ä½³å®éªŒ\n",
    "        \"\"\"\n",
    "        best_exp = None\n",
    "        best_value = None\n",
    "        \n",
    "        for exp_id, exp in self.experiments.items():\n",
    "            if metric in exp['metrics'] and exp['metrics'][metric]:\n",
    "                final_value = exp['metrics'][metric][-1]['value']\n",
    "                \n",
    "                if best_value is None:\n",
    "                    best_value = final_value\n",
    "                    best_exp = exp_id\n",
    "                elif mode == 'max' and final_value > best_value:\n",
    "                    best_value = final_value\n",
    "                    best_exp = exp_id\n",
    "                elif mode == 'min' and final_value < best_value:\n",
    "                    best_value = final_value\n",
    "                    best_exp = exp_id\n",
    "        \n",
    "        return best_exp, best_value\n",
    "\n",
    "# Example usage / ä½¿ç”¨ç¤ºä¾‹\n",
    "exp_manager = ExperimentManager()\n",
    "\n",
    "# Create experiments / åˆ›å»ºå®éªŒ\n",
    "for i in range(3):\n",
    "    config = ExperimentConfig(\n",
    "        name=f\"test_exp_{i}\",\n",
    "        model_type=\"LightGBM\",\n",
    "        dataset=\"csi300\",\n",
    "        hyperparameters={'learning_rate': 0.1 * (i + 1)},\n",
    "        metrics=['sharpe', 'return'],\n",
    "        tags=['test', 'demo']\n",
    "    )\n",
    "    \n",
    "    exp_id = exp_manager.create_experiment(config)\n",
    "    \n",
    "    # Log metrics / è®°å½•æŒ‡æ ‡\n",
    "    exp_manager.log_metric('sharpe', np.random.random() + i * 0.1)\n",
    "    exp_manager.log_metric('return', np.random.random() * 0.2)\n",
    "\n",
    "# Compare experiments / æ¯”è¾ƒå®éªŒ\n",
    "comparison = exp_manager.compare_experiments()\n",
    "print(\"\\nExperiment Comparison:\")\n",
    "print(comparison)\n",
    "\n",
    "# Find best experiment / æ‰¾åˆ°æœ€ä½³å®éªŒ\n",
    "best_exp, best_sharpe = exp_manager.get_best_experiment('sharpe', mode='max')\n",
    "print(f\"\\nBest experiment: {best_exp} with Sharpe: {best_sharpe:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Utilities / æ•°æ®å·¥å…· <a id='data-utilities'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced data utilities / é«˜çº§æ•°æ®å·¥å…·\n",
    "\n",
    "class DataUtils:\n",
    "    \"\"\"Comprehensive data utility functions\n",
    "    ç»¼åˆæ•°æ®å·¥å…·å‡½æ•°\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def resample_data(data: pd.DataFrame, source_freq: str = 'D', \n",
    "                     target_freq: str = 'W') -> pd.DataFrame:\n",
    "        \"\"\"Resample time series data / é‡é‡‡æ ·æ—¶é—´åºåˆ—æ•°æ®\"\"\"\n",
    "        if isinstance(data.index, pd.MultiIndex):\n",
    "            # Handle multi-index (datetime, instrument)\n",
    "            return data.groupby(level='instrument').resample(\n",
    "                target_freq, level='datetime'\n",
    "            ).agg({\n",
    "                col: 'last' if 'price' in col.lower() else 'sum' \n",
    "                for col in data.columns\n",
    "            })\n",
    "        else:\n",
    "            return data.resample(target_freq).last()\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_outliers(data: pd.DataFrame, method: str = 'iqr', \n",
    "                       threshold: float = 1.5) -> pd.DataFrame:\n",
    "        \"\"\"Detect outliers in data / æ£€æµ‹æ•°æ®ä¸­çš„å¼‚å¸¸å€¼\"\"\"\n",
    "        outliers = pd.DataFrame(False, index=data.index, columns=data.columns)\n",
    "        \n",
    "        if method == 'iqr':\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = (data < (Q1 - threshold * IQR)) | (data > (Q3 + threshold * IQR))\n",
    "        elif method == 'zscore':\n",
    "            z_scores = np.abs((data - data.mean()) / data.std())\n",
    "            outliers = z_scores > threshold\n",
    "        elif method == 'isolation_forest':\n",
    "            from sklearn.ensemble import IsolationForest\n",
    "            clf = IsolationForest(contamination=0.1, random_state=42)\n",
    "            outliers_pred = clf.fit_predict(data)\n",
    "            outliers = pd.DataFrame(\n",
    "                outliers_pred == -1, \n",
    "                index=data.index, \n",
    "                columns=['outlier']\n",
    "            )\n",
    "        \n",
    "        return outliers\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_lag_features(data: pd.DataFrame, lags: List[int], \n",
    "                          columns: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Create lag features / åˆ›å»ºæ»åç‰¹å¾\"\"\"\n",
    "        if columns is None:\n",
    "            columns = data.columns.tolist()\n",
    "        \n",
    "        lag_features = data.copy()\n",
    "        \n",
    "        for col in columns:\n",
    "            for lag in lags:\n",
    "                lag_features[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
    "        \n",
    "        return lag_features\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_rolling_features(data: pd.DataFrame, windows: List[int],\n",
    "                                 functions: List[str] = ['mean', 'std']) -> pd.DataFrame:\n",
    "        \"\"\"Calculate rolling window features / è®¡ç®—æ»šåŠ¨çª—å£ç‰¹å¾\"\"\"\n",
    "        rolling_features = data.copy()\n",
    "        \n",
    "        for col in data.columns:\n",
    "            for window in windows:\n",
    "                for func in functions:\n",
    "                    feature_name = f'{col}_roll_{window}_{func}'\n",
    "                    if func == 'mean':\n",
    "                        rolling_features[feature_name] = data[col].rolling(window).mean()\n",
    "                    elif func == 'std':\n",
    "                        rolling_features[feature_name] = data[col].rolling(window).std()\n",
    "                    elif func == 'max':\n",
    "                        rolling_features[feature_name] = data[col].rolling(window).max()\n",
    "                    elif func == 'min':\n",
    "                        rolling_features[feature_name] = data[col].rolling(window).min()\n",
    "        \n",
    "        return rolling_features\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=128)\n",
    "    def get_trading_calendar(market: str, start: str, end: str) -> pd.DatetimeIndex:\n",
    "        \"\"\"Get trading calendar with caching / è·å–äº¤æ˜“æ—¥å†ï¼ˆå¸¦ç¼“å­˜ï¼‰\"\"\"\n",
    "        return D.calendar(start_time=start, end_time=end, freq='day')\n",
    "\n",
    "# Example usage / ä½¿ç”¨ç¤ºä¾‹\n",
    "data_utils = DataUtils()\n",
    "\n",
    "# Create sample data / åˆ›å»ºç¤ºä¾‹æ•°æ®\n",
    "dates = pd.date_range('2024-01-01', '2024-01-31', freq='D')\n",
    "sample_data = pd.DataFrame({\n",
    "    'price': 100 + np.random.randn(len(dates)).cumsum(),\n",
    "    'volume': np.random.randint(1000, 10000, len(dates)),\n",
    "    'returns': np.random.randn(len(dates)) * 0.02\n",
    "}, index=dates)\n",
    "\n",
    "# Detect outliers / æ£€æµ‹å¼‚å¸¸å€¼\n",
    "outliers = data_utils.detect_outliers(sample_data, method='zscore', threshold=2)\n",
    "print(f\"Outliers detected: {outliers.sum().sum()}\")\n",
    "\n",
    "# Create lag features / åˆ›å»ºæ»åç‰¹å¾\n",
    "lag_data = data_utils.create_lag_features(sample_data, lags=[1, 5, 10], columns=['returns'])\n",
    "print(f\"\\nColumns after lag features: {lag_data.columns.tolist()}\")\n",
    "\n",
    "# Calculate rolling features / è®¡ç®—æ»šåŠ¨ç‰¹å¾\n",
    "rolling_data = data_utils.calculate_rolling_features(\n",
    "    sample_data[['price', 'returns']], \n",
    "    windows=[5, 20],\n",
    "    functions=['mean', 'std']\n",
    ")\n",
    "print(f\"\\nColumns after rolling features: {rolling_data.columns.tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering Helpers / ç‰¹å¾å·¥ç¨‹è¾…åŠ© <a id='feature-engineering'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced feature engineering helpers / é«˜çº§ç‰¹å¾å·¥ç¨‹è¾…åŠ©\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Advanced feature engineering toolkit\n",
    "    é«˜çº§ç‰¹å¾å·¥ç¨‹å·¥å…·åŒ…\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_importance = {}\n",
    "        self.feature_stats = {}\n",
    "    \n",
    "    def create_technical_indicators(self, ohlcv: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create comprehensive technical indicators\n",
    "        åˆ›å»ºç»¼åˆæŠ€æœ¯æŒ‡æ ‡\n",
    "        \"\"\"\n",
    "        features = ohlcv.copy()\n",
    "        \n",
    "        # Price-based indicators / åŸºäºä»·æ ¼çš„æŒ‡æ ‡\n",
    "        features['SMA_5'] = ohlcv['close'].rolling(5).mean()\n",
    "        features['SMA_20'] = ohlcv['close'].rolling(20).mean()\n",
    "        features['EMA_12'] = ohlcv['close'].ewm(span=12).mean()\n",
    "        features['EMA_26'] = ohlcv['close'].ewm(span=26).mean()\n",
    "        \n",
    "        # MACD\n",
    "        features['MACD'] = features['EMA_12'] - features['EMA_26']\n",
    "        features['MACD_signal'] = features['MACD'].ewm(span=9).mean()\n",
    "        features['MACD_hist'] = features['MACD'] - features['MACD_signal']\n",
    "        \n",
    "        # Bollinger Bands / å¸ƒæ—å¸¦\n",
    "        bb_period = 20\n",
    "        bb_std = ohlcv['close'].rolling(bb_period).std()\n",
    "        bb_mean = ohlcv['close'].rolling(bb_period).mean()\n",
    "        features['BB_upper'] = bb_mean + 2 * bb_std\n",
    "        features['BB_lower'] = bb_mean - 2 * bb_std\n",
    "        features['BB_width'] = features['BB_upper'] - features['BB_lower']\n",
    "        features['BB_position'] = (ohlcv['close'] - features['BB_lower']) / features['BB_width']\n",
    "        \n",
    "        # RSI\n",
    "        delta = ohlcv['close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "        rs = gain / loss\n",
    "        features['RSI'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Volume indicators / æˆäº¤é‡æŒ‡æ ‡\n",
    "        features['volume_ratio'] = ohlcv['volume'] / ohlcv['volume'].rolling(20).mean()\n",
    "        features['OBV'] = (np.sign(ohlcv['close'].diff()) * ohlcv['volume']).cumsum()\n",
    "        \n",
    "        # Volatility / æ³¢åŠ¨ç‡\n",
    "        features['volatility_20'] = ohlcv['close'].pct_change().rolling(20).std()\n",
    "        features['ATR'] = self._calculate_atr(ohlcv)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _calculate_atr(self, ohlcv: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "        \"\"\"Calculate Average True Range / è®¡ç®—å¹³å‡çœŸå®èŒƒå›´\"\"\"\n",
    "        high_low = ohlcv['high'] - ohlcv['low']\n",
    "        high_close = np.abs(ohlcv['high'] - ohlcv['close'].shift())\n",
    "        low_close = np.abs(ohlcv['low'] - ohlcv['close'].shift())\n",
    "        \n",
    "        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "        return true_range.rolling(period).mean()\n",
    "    \n",
    "    def create_interaction_features(self, data: pd.DataFrame, \n",
    "                                  max_degree: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"Create polynomial and interaction features\n",
    "        åˆ›å»ºå¤šé¡¹å¼å’Œäº¤äº’ç‰¹å¾\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        \n",
    "        poly = PolynomialFeatures(degree=max_degree, include_bias=False)\n",
    "        poly_features = poly.fit_transform(data)\n",
    "        \n",
    "        feature_names = poly.get_feature_names_out(data.columns)\n",
    "        return pd.DataFrame(poly_features, columns=feature_names, index=data.index)\n",
    "    \n",
    "    def select_features(self, X: pd.DataFrame, y: pd.Series, \n",
    "                       method: str = 'mutual_info', k: int = 20) -> List[str]:\n",
    "        \"\"\"Feature selection using various methods\n",
    "        ä½¿ç”¨å„ç§æ–¹æ³•è¿›è¡Œç‰¹å¾é€‰æ‹©\n",
    "        \"\"\"\n",
    "        from sklearn.feature_selection import (\n",
    "            SelectKBest, mutual_info_regression, f_regression\n",
    "        )\n",
    "        \n",
    "        if method == 'mutual_info':\n",
    "            selector = SelectKBest(mutual_info_regression, k=k)\n",
    "        elif method == 'f_regression':\n",
    "            selector = SelectKBest(f_regression, k=k)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        selector.fit(X, y)\n",
    "        selected_features = X.columns[selector.get_support()].tolist()\n",
    "        \n",
    "        # Store feature importance / å­˜å‚¨ç‰¹å¾é‡è¦æ€§\n",
    "        self.feature_importance[method] = dict(zip(X.columns, selector.scores_))\n",
    "        \n",
    "        return selected_features\n",
    "    \n",
    "    def create_target_encoding(self, data: pd.DataFrame, \n",
    "                              categorical_cols: List[str],\n",
    "                              target_col: str) -> pd.DataFrame:\n",
    "        \"\"\"Target encoding for categorical features\n",
    "        åˆ†ç±»ç‰¹å¾çš„ç›®æ ‡ç¼–ç \n",
    "        \"\"\"\n",
    "        encoded_data = data.copy()\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            # Calculate mean target for each category\n",
    "            target_mean = data.groupby(col)[target_col].mean()\n",
    "            \n",
    "            # Add noise to prevent overfitting\n",
    "            noise_level = 0.01\n",
    "            target_mean += np.random.normal(0, noise_level, len(target_mean))\n",
    "            \n",
    "            # Map to original data\n",
    "            encoded_data[f'{col}_target_enc'] = data[col].map(target_mean)\n",
    "        \n",
    "        return encoded_data\n",
    "\n",
    "# Example usage / ä½¿ç”¨ç¤ºä¾‹\n",
    "fe = FeatureEngineer()\n",
    "\n",
    "# Create sample OHLCV data / åˆ›å»ºç¤ºä¾‹OHLCVæ•°æ®\n",
    "dates = pd.date_range('2024-01-01', '2024-03-31', freq='D')\n",
    "ohlcv = pd.DataFrame({\n",
    "    'open': 100 + np.random.randn(len(dates)).cumsum(),\n",
    "    'high': 102 + np.random.randn(len(dates)).cumsum(),\n",
    "    'low': 98 + np.random.randn(len(dates)).cumsum(),\n",
    "    'close': 100 + np.random.randn(len(dates)).cumsum(),\n",
    "    'volume': np.random.randint(1000000, 5000000, len(dates))\n",
    "}, index=dates)\n",
    "\n",
    "# Create technical indicators / åˆ›å»ºæŠ€æœ¯æŒ‡æ ‡\n",
    "tech_features = fe.create_technical_indicators(ohlcv)\n",
    "print(f\"Technical features created: {len(tech_features.columns)} columns\")\n",
    "print(f\"Sample features: {tech_features.columns.tolist()[:10]}...\")\n",
    "\n",
    "# Feature selection example / ç‰¹å¾é€‰æ‹©ç¤ºä¾‹\n",
    "X = tech_features.dropna()\n",
    "y = X['close'].pct_change().shift(-1).fillna(0)  # Next day return as target\n",
    "selected = fe.select_features(X, y, method='f_regression', k=10)\n",
    "print(f\"\\nTop 10 selected features: {selected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Advanced Features / é«˜çº§åŠŸèƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Market Support / å¤šå¸‚åœºæ”¯æŒ <a id='multi-market'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-market trading framework / å¤šå¸‚åœºäº¤æ˜“æ¡†æ¶\n",
    "\n",
    "class MultiMarketFramework:\n",
    "    \"\"\"Framework for multi-market trading\n",
    "    å¤šå¸‚åœºäº¤æ˜“æ¡†æ¶\n",
    "    \"\"\"\n",
    "    \n",
    "    # Market configurations / å¸‚åœºé…ç½®\n",
    "    MARKET_CONFIG = {\n",
    "        'CN': {\n",
    "            'name': 'China A-Share',\n",
    "            'trading_hours': '09:30-15:00',\n",
    "            'timezone': 'Asia/Shanghai',\n",
    "            'currency': 'CNY',\n",
    "            'indices': ['csi300', 'csi500', 'csi1000'],\n",
    "            'limit': 0.10,  # 10% price limit\n",
    "        },\n",
    "        'US': {\n",
    "            'name': 'US Market',\n",
    "            'trading_hours': '09:30-16:00',\n",
    "            'timezone': 'America/New_York',\n",
    "            'currency': 'USD',\n",
    "            'indices': ['sp500', 'nasdaq100', 'russell2000'],\n",
    "            'limit': None,  # No price limit\n",
    "        },\n",
    "        'HK': {\n",
    "            'name': 'Hong Kong Market',\n",
    "            'trading_hours': '09:30-16:00',\n",
    "            'timezone': 'Asia/Hong_Kong',\n",
    "            'currency': 'HKD',\n",
    "            'indices': ['hsi', 'hscei'],\n",
    "            'limit': None,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, markets: List[str]):\n",
    "        self.markets = markets\n",
    "        self.market_data = {}\n",
    "        self.portfolios = {}\n",
    "        \n",
    "    def initialize_markets(self):\n",
    "        \"\"\"Initialize multiple markets / åˆå§‹åŒ–å¤šä¸ªå¸‚åœº\"\"\"\n",
    "        for market in self.markets:\n",
    "            if market in self.MARKET_CONFIG:\n",
    "                config = self.MARKET_CONFIG[market]\n",
    "                print(f\"Initializing {config['name']}...\")\n",
    "                \n",
    "                # Initialize market-specific components\n",
    "                self.portfolios[market] = {\n",
    "                    'positions': {},\n",
    "                    'cash': 1000000,  # Initial cash\n",
    "                    'currency': config['currency']\n",
    "                }\n",
    "    \n",
    "    def convert_currency(self, amount: float, from_currency: str, \n",
    "                        to_currency: str) -> float:\n",
    "        \"\"\"Currency conversion / è´§å¸è½¬æ¢\"\"\"\n",
    "        # Simplified exchange rates / ç®€åŒ–çš„æ±‡ç‡\n",
    "        exchange_rates = {\n",
    "            'CNY_USD': 0.14,\n",
    "            'USD_CNY': 7.2,\n",
    "            'HKD_USD': 0.13,\n",
    "            'USD_HKD': 7.8,\n",
    "            'CNY_HKD': 1.1,\n",
    "            'HKD_CNY': 0.92\n",
    "        }\n",
    "        \n",
    "        if from_currency == to_currency:\n",
    "            return amount\n",
    "        \n",
    "        rate_key = f\"{from_currency}_{to_currency}\"\n",
    "        if rate_key in exchange_rates:\n",
    "            return amount * exchange_rates[rate_key]\n",
    "        \n",
    "        return amount\n",
    "    \n",
    "    def aggregate_portfolio(self, base_currency: str = 'USD') -> dict:\n",
    "        \"\"\"Aggregate multi-market portfolio / èšåˆå¤šå¸‚åœºç»„åˆ\"\"\"\n",
    "        total_value = 0\n",
    "        aggregated = {'positions': {}, 'cash': 0, 'currency': base_currency}\n",
    "        \n",
    "        for market, portfolio in self.portfolios.items():\n",
    "            market_config = self.MARKET_CONFIG[market]\n",
    "            \n",
    "            # Convert cash to base currency\n",
    "            cash_in_base = self.convert_currency(\n",
    "                portfolio['cash'],\n",
    "                market_config['currency'],\n",
    "                base_currency\n",
    "            )\n",
    "            aggregated['cash'] += cash_in_base\n",
    "            \n",
    "            # Aggregate positions\n",
    "            for stock, position in portfolio['positions'].items():\n",
    "                key = f\"{market}:{stock}\"\n",
    "                aggregated['positions'][key] = position\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def cross_market_arbitrage(self, symbol: str) -> dict:\n",
    "        \"\"\"Identify cross-market arbitrage opportunities\n",
    "        è¯†åˆ«è·¨å¸‚åœºå¥—åˆ©æœºä¼š\n",
    "        \"\"\"\n",
    "        opportunities = []\n",
    "        \n",
    "        # Example: A+H share arbitrage\n",
    "        # Check if stock is listed in multiple markets\n",
    "        prices = {}\n",
    "        for market in self.markets:\n",
    "            # Simulate price fetching\n",
    "            prices[market] = np.random.uniform(90, 110)\n",
    "        \n",
    "        # Find arbitrage opportunities\n",
    "        for m1 in self.markets:\n",
    "            for m2 in self.markets:\n",
    "                if m1 != m2:\n",
    "                    price_diff = abs(prices[m1] - prices[m2]) / prices[m1]\n",
    "                    if price_diff > 0.02:  # 2% threshold\n",
    "                        opportunities.append({\n",
    "                            'symbol': symbol,\n",
    "                            'market_1': m1,\n",
    "                            'market_2': m2,\n",
    "                            'price_1': prices[m1],\n",
    "                            'price_2': prices[m2],\n",
    "                            'spread': price_diff\n",
    "                        })\n",
    "        \n",
    "        return opportunities\n",
    "\n",
    "# Example usage / ä½¿ç”¨ç¤ºä¾‹\n",
    "multi_market = MultiMarketFramework(['CN', 'US', 'HK'])\n",
    "multi_market.initialize_markets()\n",
    "\n",
    "# Aggregate portfolio / èšåˆç»„åˆ\n",
    "aggregated = multi_market.aggregate_portfolio(base_currency='USD')\n",
    "print(f\"\\nAggregated portfolio in USD:\")\n",
    "print(f\"  Total cash: ${aggregated['cash']:,.2f}\")\n",
    "\n",
    "# Check arbitrage opportunities / æ£€æŸ¥å¥—åˆ©æœºä¼š\n",
    "arb_ops = multi_market.cross_market_arbitrage('BABA')\n",
    "if arb_ops:\n",
    "    print(f\"\\nArbitrage opportunities found:\")\n",
    "    for op in arb_ops:\n",
    "        print(f\"  {op['symbol']}: {op['market_1']} vs {op['market_2']}, spread: {op['spread']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Online Learning / åœ¨çº¿å­¦ä¹  <a id='online-learning'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning framework / åœ¨çº¿å­¦ä¹ æ¡†æ¶\n",
    "\n",
    "class OnlineLearningFramework:\n",
    "    \"\"\"Framework for online/incremental learning\n",
    "    åœ¨çº¿/å¢é‡å­¦ä¹ æ¡†æ¶\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, update_frequency: int = 100):\n",
    "        self.base_model = base_model\n",
    "        self.update_frequency = update_frequency\n",
    "        self.data_buffer = []\n",
    "        self.performance_history = []\n",
    "        self.update_count = 0\n",
    "        \n",
    "    def update_model(self, new_data: pd.DataFrame, new_labels: pd.Series):\n",
    "        \"\"\"Incrementally update the model / å¢é‡æ›´æ–°æ¨¡å‹\"\"\"\n",
    "        # Add to buffer\n",
    "        self.data_buffer.append((new_data, new_labels))\n",
    "        \n",
    "        # Check if update is needed\n",
    "        if len(self.data_buffer) >= self.update_frequency:\n",
    "            print(f\"Updating model (update #{self.update_count + 1})...\")\n",
    "            \n",
    "            # Combine buffered data\n",
    "            X = pd.concat([d[0] for d in self.data_buffer])\n",
    "            y = pd.concat([d[1] for d in self.data_buffer])\n",
    "            \n",
    "            # Partial fit (for models that support it)\n",
    "            if hasattr(self.base_model, 'partial_fit'):\n",
    "                self.base_model.partial_fit(X, y)\n",
    "            else:\n",
    "                # Retrain on recent data\n",
    "                self.base_model.fit(X, y)\n",
    "            \n",
    "            # Clear buffer\n",
    "            self.data_buffer = []\n",
    "            self.update_count += 1\n",
    "            \n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def adaptive_learning_rate(self, performance_metric: float):\n",
    "        \"\"\"Adjust learning rate based on performance\n",
    "        åŸºäºæ€§èƒ½è°ƒæ•´å­¦ä¹ ç‡\n",
    "        \"\"\"\n",
    "        self.performance_history.append(performance_metric)\n",
    "        \n",
    "        if len(self.performance_history) > 10:\n",
    "            recent_performance = self.performance_history[-10:]\n",
    "            performance_trend = np.polyfit(range(10), recent_performance, 1)[0]\n",
    "            \n",
    "            # Adjust learning rate based on trend\n",
    "            if hasattr(self.base_model, 'learning_rate'):\n",
    "                if performance_trend < 0:  # Performance declining\n",
    "                    self.base_model.learning_rate *= 0.9\n",
    "                    print(f\"Decreased learning rate to {self.base_model.learning_rate:.4f}\")\n",
    "                elif performance_trend > 0.01:  # Performance improving\n",
    "                    self.base_model.learning_rate *= 1.1\n",
    "                    print(f\"Increased learning rate to {self.base_model.learning_rate:.4f}\")\n",
    "    \n",
    "    def concept_drift_detection(self, predictions: np.ndarray, \n",
    "                              actuals: np.ndarray,\n",
    "                              threshold: float = 0.1):\n",
    "        \"\"\"Detect concept drift in data / æ£€æµ‹æ•°æ®ä¸­çš„æ¦‚å¿µæ¼‚ç§»\"\"\"\n",
    "        error = np.abs(predictions - actuals).mean()\n",
    "        \n",
    "        if not hasattr(self, 'baseline_error'):\n",
    "            self.baseline_error = error\n",
    "            return False\n",
    "        \n",
    "        drift_ratio = (error - self.baseline_error) / self.baseline_error\n",
    "        \n",
    "        if drift_ratio > threshold:\n",
    "            print(f\"âš ï¸ Concept drift detected! Error increased by {drift_ratio:.2%}\")\n",
    "            return True\n",
    "        \n",
    "        # Update baseline with exponential moving average\n",
    "        self.baseline_error = 0.9 * self.baseline_error + 0.1 * error\n",
    "        return False\n",
    "\n",
    "# Example usage / ä½¿ç”¨ç¤ºä¾‹\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Create online learning model / åˆ›å»ºåœ¨çº¿å­¦ä¹ æ¨¡å‹\n",
    "base_model = SGDRegressor(learning_rate='constant', eta0=0.01)\n",
    "online_learner = OnlineLearningFramework(base_model, update_frequency=50)\n",
    "\n",
    "# Simulate streaming data / æ¨¡æ‹Ÿæµæ•°æ®\n",
    "for i in range(200):\n",
    "    # Generate new data batch\n",
    "    X_new = pd.DataFrame(np.random.randn(10, 5), columns=[f'f{j}' for j in range(5)])\n",
    "    y_new = pd.Series(np.random.randn(10))\n",
    "    \n",
    "    # Update model\n",
    "    updated = online_learner.update_model(X_new, y_new)\n",
    "    \n",
    "    if updated:\n",
    "        # Make predictions and check for drift\n",
    "        predictions = base_model.predict(X_new)\n",
    "        drift = online_learner.concept_drift_detection(predictions, y_new.values)\n",
    "        \n",
    "        if drift:\n",
    "            print(\"  Triggering model retraining due to concept drift...\")\n",
    "\n",
    "print(f\"\\nTotal model updates: {online_learner.update_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Production Tools / ç”Ÿäº§å·¥å…·"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-time Trading Integration / å®æ—¶äº¤æ˜“é›†æˆ <a id='realtime-trading'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time trading system / å®æ—¶äº¤æ˜“ç³»ç»Ÿ\n",
    "\n",
    "class RealTimeTradingSystem:\n",
    "    \"\"\"Real-time trading system with risk management\n",
    "    å¸¦é£é™©ç®¡ç†çš„å®æ—¶äº¤æ˜“ç³»ç»Ÿ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, risk_manager=None):\n",
    "        self.model = model\n",
    "        self.risk_manager = risk_manager\n",
    "        self.order_queue = []\n",
    "        self.positions = {}\n",
    "        self.trading_status = 'STOPPED'\n",
    "        self.last_update = None\n",
    "        \n",
    "    def start_trading(self):\n",
    "        \"\"\"Start real-time trading / å¼€å§‹å®æ—¶äº¤æ˜“\"\"\"\n",
    "        self.trading_status = 'RUNNING'\n",
    "        print(\"ğŸŸ¢ Trading system started\")\n",
    "        \n",
    "        # Start event loop (simplified)\n",
    "        self._trading_loop()\n",
    "    \n",
    "    def stop_trading(self):\n",
    "        \"\"\"Stop trading / åœæ­¢äº¤æ˜“\"\"\"\n",
    "        self.trading_status = 'STOPPED'\n",
    "        print(\"ğŸ”´ Trading system stopped\")\n",
    "    \n",
    "    def _trading_loop(self):\n",
    "        \"\"\"Main trading loop / ä¸»äº¤æ˜“å¾ªç¯\"\"\"\n",
    "        import asyncio\n",
    "        \n",
    "        async def process_market_data():\n",
    "            while self.trading_status == 'RUNNING':\n",
    "                # Fetch real-time data (simulated)\n",
    "                market_data = self._fetch_market_data()\n",
    "                \n",
    "                # Generate signals\n",
    "                signals = self._generate_signals(market_data)\n",
    "                \n",
    "                # Risk check\n",
    "                if self.risk_manager:\n",
    "                    signals = self.risk_manager.check_signals(signals, self.positions)\n",
    "                \n",
    "                # Generate orders\n",
    "                orders = self._generate_orders(signals)\n",
    "                \n",
    "                # Execute orders\n",
    "                await self._execute_orders(orders)\n",
    "                \n",
    "                # Update positions\n",
    "                self._update_positions()\n",
    "                \n",
    "                await asyncio.sleep(1)  # Wait 1 second\n",
    "        \n",
    "        # Run async loop (simplified for demo)\n",
    "        print(\"Trading loop simulation...\")\n",
    "        for _ in range(3):  # Simulate 3 iterations\n",
    "            market_data = self._fetch_market_data()\n",
    "            signals = self._generate_signals(market_data)\n",
    "            print(f\"  Generated {len(signals)} signals\")\n",
    "            time.sleep(1)\n",
    "    \n",
    "    def _fetch_market_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Fetch real-time market data / è·å–å®æ—¶å¸‚åœºæ•°æ®\"\"\"\n",
    "        # Simulated real-time data\n",
    "        stocks = ['AAPL', 'GOOGL', 'MSFT', 'AMZN']\n",
    "        data = pd.DataFrame({\n",
    "            'symbol': stocks,\n",
    "            'price': np.random.uniform(100, 200, len(stocks)),\n",
    "            'volume': np.random.randint(1000000, 10000000, len(stocks)),\n",
    "            'bid': np.random.uniform(99, 199, len(stocks)),\n",
    "            'ask': np.random.uniform(101, 201, len(stocks))\n",
    "        })\n",
    "        self.last_update = datetime.now()\n",
    "        return data\n",
    "    \n",
    "    def _generate_signals(self, market_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Generate trading signals / ç”Ÿæˆäº¤æ˜“ä¿¡å·\"\"\"\n",
    "        # Use model to generate signals\n",
    "        signals = pd.DataFrame({\n",
    "            'symbol': market_data['symbol'],\n",
    "            'signal': np.random.uniform(-1, 1, len(market_data)),\n",
    "            'confidence': np.random.uniform(0.5, 1.0, len(market_data))\n",
    "        })\n",
    "        return signals\n",
    "    \n",
    "    def _generate_orders(self, signals: pd.DataFrame) -> List[dict]:\n",
    "        \"\"\"Generate orders from signals / ä»ä¿¡å·ç”Ÿæˆè®¢å•\"\"\"\n",
    "        orders = []\n",
    "        \n",
    "        for _, signal in signals.iterrows():\n",
    "            if abs(signal['signal']) > 0.5 and signal['confidence'] > 0.7:\n",
    "                order = {\n",
    "                    'symbol': signal['symbol'],\n",
    "                    'side': 'BUY' if signal['signal'] > 0 else 'SELL',\n",
    "                    'quantity': int(abs(signal['signal']) * 100),\n",
    "                    'order_type': 'MARKET',\n",
    "                    'timestamp': datetime.now()\n",
    "                }\n",
    "                orders.append(order)\n",
    "        \n",
    "        return orders\n",
    "    \n",
    "    async def _execute_orders(self, orders: List[dict]):\n",
    "        \"\"\"Execute orders / æ‰§è¡Œè®¢å•\"\"\"\n",
    "        for order in orders:\n",
    "            print(f\"  Executing {order['side']} {order['quantity']} {order['symbol']}\")\n",
    "            self.order_queue.append(order)\n",
    "    \n",
    "    def _update_positions(self):\n",
    "        \"\"\"Update positions / æ›´æ–°æŒä»“\"\"\"\n",
    "        for order in self.order_queue:\n",
    "            symbol = order['symbol']\n",
    "            quantity = order['quantity'] if order['side'] == 'BUY' else -order['quantity']\n",
    "            \n",
    "            if symbol in self.positions:\n",
    "                self.positions[symbol] += quantity\n",
    "            else:\n",
    "                self.positions[symbol] = quantity\n",
    "        \n",
    "        self.order_queue = []\n",
    "\n",
    "class RiskManager:\n",
    "    \"\"\"Risk management system / é£é™©ç®¡ç†ç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    def __init__(self, max_position_size: float = 0.1, \n",
    "                 max_total_exposure: float = 0.8):\n",
    "        self.max_position_size = max_position_size\n",
    "        self.max_total_exposure = max_total_exposure\n",
    "        \n",
    "    def check_signals(self, signals: pd.DataFrame, \n",
    "                     current_positions: dict) -> pd.DataFrame:\n",
    "        \"\"\"Check signals against risk limits / æ ¹æ®é£é™©é™åˆ¶æ£€æŸ¥ä¿¡å·\"\"\"\n",
    "        # Apply position size limits\n",
    "        signals['signal'] = signals['signal'].clip(-self.max_position_size, \n",
    "                                                   self.max_position_size)\n",
    "        \n",
    "        # Check total exposure\n",
    "        total_exposure = sum(abs(v) for v in current_positions.values())\n",
    "        if total_exposure > self.max_total_exposure:\n",
    "            print(\"âš ï¸ Risk limit reached, scaling down signals\")\n",
    "            signals['signal'] *= 0.5\n",
    "        \n",
    "        return signals\n",
    "\n",
    "# Example usage / ä½¿ç”¨ç¤ºä¾‹\n",
    "risk_mgr = RiskManager(max_position_size=0.1, max_total_exposure=0.8)\n",
    "trading_system = RealTimeTradingSystem(model=None, risk_manager=risk_mgr)\n",
    "\n",
    "# Start trading\n",
    "trading_system.start_trading()\n",
    "\n",
    "# Check positions\n",
    "print(f\"\\nCurrent positions: {trading_system.positions}\")\n",
    "print(f\"Last update: {trading_system.last_update}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Monitoring and Alerting / ç›‘æ§ä¸å‘Šè­¦ <a id='monitoring'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring and alerting system / ç›‘æ§ä¸å‘Šè­¦ç³»ç»Ÿ\n",
    "\n",
    "class MonitoringSystem:\n",
    "    \"\"\"Comprehensive monitoring and alerting system\n",
    "    ç»¼åˆç›‘æ§ä¸å‘Šè­¦ç³»ç»Ÿ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alert_config: dict = None):\n",
    "        self.metrics = {}\n",
    "        self.alerts = []\n",
    "        self.alert_config = alert_config or self._default_alert_config()\n",
    "        self.alert_handlers = []\n",
    "        \n",
    "    def _default_alert_config(self) -> dict:\n",
    "        \"\"\"Default alert configuration / é»˜è®¤å‘Šè­¦é…ç½®\"\"\"\n",
    "        return {\n",
    "            'max_drawdown': -0.10,\n",
    "            'daily_loss_limit': -0.05,\n",
    "            'position_limit': 0.20,\n",
    "            'volatility_threshold': 0.30,\n",
    "            'sharpe_threshold': 0.5\n",
    "        }\n",
    "    \n",
    "    def track_metric(self, name: str, value: float, timestamp: datetime = None):\n",
    "        \"\"\"Track a metric / è·Ÿè¸ªæŒ‡æ ‡\"\"\"\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now()\n",
    "        \n",
    "        if name not in self.metrics:\n",
    "            self.metrics[name] = []\n",
    "        \n",
    "        self.metrics[name].append({'value': value, 'timestamp': timestamp})\n",
    "        \n",
    "        # Check for alerts\n",
    "        self._check_alerts(name, value)\n",
    "    \n",
    "    def _check_alerts(self, metric_name: str, value: float):\n",
    "        \"\"\"Check if alerts should be triggered / æ£€æŸ¥æ˜¯å¦åº”è§¦å‘å‘Šè­¦\"\"\"\n",
    "        alerts_triggered = []\n",
    "        \n",
    "        # Drawdown alert\n",
    "        if metric_name == 'drawdown' and value < self.alert_config['max_drawdown']:\n",
    "            alerts_triggered.append({\n",
    "                'type': 'CRITICAL',\n",
    "                'metric': metric_name,\n",
    "                'value': value,\n",
    "                'threshold': self.alert_config['max_drawdown'],\n",
    "                'message': f\"Drawdown {value:.2%} exceeds limit {self.alert_config['max_drawdown']:.2%}\"\n",
    "            })\n",
    "        \n",
    "        # Daily loss alert\n",
    "        if metric_name == 'daily_return' and value < self.alert_config['daily_loss_limit']:\n",
    "            alerts_triggered.append({\n",
    "                'type': 'WARNING',\n",
    "                'metric': metric_name,\n",
    "                'value': value,\n",
    "                'threshold': self.alert_config['daily_loss_limit'],\n",
    "                'message': f\"Daily loss {value:.2%} exceeds limit\"\n",
    "            })\n",
    "        \n",
    "        # Process alerts\n",
    "        for alert in alerts_triggered:\n",
    "            self._trigger_alert(alert)\n",
    "    \n",
    "    def _trigger_alert(self, alert: dict):\n",
    "        \"\"\"Trigger an alert / è§¦å‘å‘Šè­¦\"\"\"\n",
    "        alert['timestamp'] = datetime.now()\n",
    "        self.alerts.append(alert)\n",
    "        \n",
    "        # Print alert\n",
    "        icon = 'ğŸ”´' if alert['type'] == 'CRITICAL' else 'âš ï¸'\n",
    "        print(f\"{icon} {alert['type']}: {alert['message']}\")\n",
    "        \n",
    "        # Call alert handlers\n",
    "        for handler in self.alert_handlers:\n",
    "            handler(alert)\n",
    "    \n",
    "    def add_alert_handler(self, handler):\n",
    "        \"\"\"Add custom alert handler / æ·»åŠ è‡ªå®šä¹‰å‘Šè­¦å¤„ç†å™¨\"\"\"\n",
    "        self.alert_handlers.append(handler)\n",
    "    \n",
    "    def generate_report(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate monitoring report / ç”Ÿæˆç›‘æ§æŠ¥å‘Š\"\"\"\n",
    "        report_data = []\n",
    "        \n",
    "        for metric_name, values in self.metrics.items():\n",
    "            if values:\n",
    "                recent_values = [v['value'] for v in values[-100:]]  # Last 100\n",
    "                report_data.append({\n",
    "                    'Metric': metric_name,\n",
    "                    'Current': values[-1]['value'],\n",
    "                    'Mean': np.mean(recent_values),\n",
    "                    'Std': np.std(recent_values),\n",
    "                    'Min': np.min(recent_values),\n",
    "                    'Max': np.max(recent_values),\n",
    "                    'Count': len(values)\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(report_data)\n",
    "    \n",
    "    def plot_metrics(self, metrics: List[str] = None):\n",
    "        \"\"\"Plot monitored metrics / ç»˜åˆ¶ç›‘æ§æŒ‡æ ‡\"\"\"\n",
    "        if metrics is None:\n",
    "            metrics = list(self.metrics.keys())\n",
    "        \n",
    "        n_metrics = len(metrics)\n",
    "        if n_metrics == 0:\n",
    "            print(\"No metrics to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(n_metrics, 1, figsize=(12, 4*n_metrics))\n",
    "        if n_metrics == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, metric_name in zip(axes, metrics):\n",
    "            if metric_name in self.metrics:\n",
    "                values = self.metrics[metric_name]\n",
    "                timestamps = [v['timestamp'] for v in values]\n",
    "                metric_values = [v['value'] for v in values]\n",
    "                \n",
    "                ax.plot(timestamps, metric_values, linewidth=2)\n",
    "                ax.set_title(f'{metric_name} Monitoring')\n",
    "                ax.set_xlabel('Time')\n",
    "                ax.set_ylabel('Value')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add threshold lines if applicable\n",
    "                if metric_name == 'drawdown':\n",
    "                    ax.axhline(y=self.alert_config['max_drawdown'], \n",
    "                             color='red', linestyle='--', alpha=0.5,\n",
    "                             label='Alert Threshold')\n",
    "                    ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage / ä½¿ç”¨ç¤ºä¾‹\n",
    "monitor = MonitoringSystem()\n",
    "\n",
    "# Add custom alert handler / æ·»åŠ è‡ªå®šä¹‰å‘Šè­¦å¤„ç†å™¨\n",
    "def email_alert_handler(alert):\n",
    "    print(f\"  ğŸ“§ Sending email alert: {alert['message']}\")\n",
    "\n",
    "monitor.add_alert_handler(email_alert_handler)\n",
    "\n",
    "# Simulate monitoring / æ¨¡æ‹Ÿç›‘æ§\n",
    "for i in range(20):\n",
    "    # Track metrics\n",
    "    monitor.track_metric('drawdown', -np.random.uniform(0, 0.15))\n",
    "    monitor.track_metric('daily_return', np.random.uniform(-0.08, 0.08))\n",
    "    monitor.track_metric('sharpe_ratio', np.random.uniform(0.3, 1.5))\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Generate report / ç”ŸæˆæŠ¥å‘Š\n",
    "report = monitor.generate_report()\n",
    "print(\"\\nMonitoring Report:\")\n",
    "print(report.round(4))\n",
    "\n",
    "# Plot metrics / ç»˜åˆ¶æŒ‡æ ‡\n",
    "monitor.plot_metrics(['drawdown', 'daily_return'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Best Practices / æœ€ä½³å®è·µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Common Pitfalls and Solutions / å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ <a id='pitfalls'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common pitfalls and solutions / å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "class QlibBestPractices:\n",
    "    \"\"\"Collection of best practices and solutions\n",
    "    æœ€ä½³å®è·µå’Œè§£å†³æ–¹æ¡ˆé›†åˆ\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def avoid_look_ahead_bias(data: pd.DataFrame, shift_days: int = 1) -> pd.DataFrame:\n",
    "        \"\"\"Avoid look-ahead bias in features\n",
    "        é¿å…ç‰¹å¾ä¸­çš„æœªæ¥å‡½æ•°\n",
    "        \"\"\"\n",
    "        # Shift all features to avoid using future information\n",
    "        feature_cols = [col for col in data.columns if col != 'label']\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            data[col] = data[col].shift(shift_days)\n",
    "        \n",
    "        return data.dropna()\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_missing_data(data: pd.DataFrame, method: str = 'forward_fill') -> pd.DataFrame:\n",
    "        \"\"\"Properly handle missing data\n",
    "        æ­£ç¡®å¤„ç†ç¼ºå¤±æ•°æ®\n",
    "        \"\"\"\n",
    "        if method == 'forward_fill':\n",
    "            # Forward fill then backward fill\n",
    "            data = data.fillna(method='ffill').fillna(method='bfill')\n",
    "        elif method == 'interpolate':\n",
    "            data = data.interpolate(method='linear')\n",
    "        elif method == 'drop':\n",
    "            data = data.dropna()\n",
    "        elif method == 'mean':\n",
    "            data = data.fillna(data.mean())\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_data_alignment(features: pd.DataFrame, \n",
    "                              labels: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Ensure features and labels are properly aligned\n",
    "        ç¡®ä¿ç‰¹å¾å’Œæ ‡ç­¾æ­£ç¡®å¯¹é½\n",
    "        \"\"\"\n",
    "        # Get common index\n",
    "        common_index = features.index.intersection(labels.index)\n",
    "        \n",
    "        if len(common_index) < len(features):\n",
    "            print(f\"âš ï¸ Warning: {len(features) - len(common_index)} samples dropped due to misalignment\")\n",
    "        \n",
    "        return features.loc[common_index], labels.loc[common_index]\n",
    "    \n",
    "    @staticmethod\n",
    "    def memory_efficient_loading(instruments: List[str], \n",
    "                               batch_size: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"Load data in batches to save memory\n",
    "        åˆ†æ‰¹åŠ è½½æ•°æ®ä»¥èŠ‚çœå†…å­˜\n",
    "        \"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        for i in range(0, len(instruments), batch_size):\n",
    "            batch = instruments[i:i+batch_size]\n",
    "            batch_data = D.features(\n",
    "                instruments=batch,\n",
    "                fields=['$close', '$volume'],\n",
    "                start_time='2024-01-01',\n",
    "                end_time='2024-01-31'\n",
    "            )\n",
    "            all_data.append(batch_data)\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if i % (batch_size * 5) == 0:\n",
    "                import gc\n",
    "                gc.collect()\n",
    "        \n",
    "        return pd.concat(all_data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def debug_model_predictions(model, dataset, sample_size: int = 10):\n",
    "        \"\"\"Debug model predictions\n",
    "        è°ƒè¯•æ¨¡å‹é¢„æµ‹\n",
    "        \"\"\"\n",
    "        # Get sample data\n",
    "        sample_data = dataset.prepare('test', col_set=['feature', 'label']).head(sample_size)\n",
    "        \n",
    "        # Make predictions\n",
    "        X = sample_data.iloc[:, :-1]\n",
    "        y_true = sample_data.iloc[:, -1]\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = model.predict(X) if hasattr(model, 'predict') else None\n",
    "        \n",
    "        # Debug output\n",
    "        debug_df = pd.DataFrame({\n",
    "            'True': y_true,\n",
    "            'Predicted': y_pred,\n",
    "            'Error': y_pred - y_true if y_pred is not None else None\n",
    "        })\n",
    "        \n",
    "        print(\"Debug Output:\")\n",
    "        print(debug_df)\n",
    "        \n",
    "        # Check for common issues\n",
    "        if y_pred is not None:\n",
    "            if np.all(y_pred == y_pred[0]):\n",
    "                print(\"âš ï¸ Warning: Model predicting constant values!\")\n",
    "            if np.any(np.isnan(y_pred)):\n",
    "                print(\"âš ï¸ Warning: Model producing NaN predictions!\")\n",
    "            if np.any(np.isinf(y_pred)):\n",
    "                print(\"âš ï¸ Warning: Model producing infinite predictions!\")\n",
    "\n",
    "# Example usage / ä½¿ç”¨ç¤ºä¾‹\n",
    "best_practices = QlibBestPractices()\n",
    "\n",
    "# Create sample data with issues / åˆ›å»ºæœ‰é—®é¢˜çš„æ ·æœ¬æ•°æ®\n",
    "dates = pd.date_range('2024-01-01', '2024-01-31')\n",
    "problem_data = pd.DataFrame({\n",
    "    'feature1': np.random.randn(len(dates)),\n",
    "    'feature2': np.random.randn(len(dates)),\n",
    "    'label': np.random.randn(len(dates))\n",
    "}, index=dates)\n",
    "\n",
    "# Add some NaN values\n",
    "problem_data.iloc[5:8, 0] = np.nan\n",
    "problem_data.iloc[15:17, 1] = np.nan\n",
    "\n",
    "print(\"Original data with NaN:\")\n",
    "print(f\"  NaN count: {problem_data.isna().sum().sum()}\")\n",
    "\n",
    "# Fix missing data\n",
    "fixed_data = best_practices.handle_missing_data(problem_data, method='forward_fill')\n",
    "print(f\"\\nAfter fixing:\")\n",
    "print(f\"  NaN count: {fixed_data.isna().sum().sum()}\")\n",
    "\n",
    "# Avoid look-ahead bias\n",
    "safe_data = best_practices.avoid_look_ahead_bias(fixed_data, shift_days=1)\n",
    "print(f\"\\nAfter avoiding look-ahead bias:\")\n",
    "print(f\"  Data shape: {safe_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary / æ€»ç»“\n",
    "\n",
    "### What we covered / æœ¬ç« å†…å®¹\n",
    "\n",
    "#### Part 1: Core Utilities / æ ¸å¿ƒå·¥å…·\n",
    "- **Workflow Automation**: Pipeline creation and management / ç®¡é“åˆ›å»ºå’Œç®¡ç†\n",
    "- **Experiment Management**: Tracking and comparing experiments / è·Ÿè¸ªå’Œæ¯”è¾ƒå®éªŒ\n",
    "- **Data Utilities**: Advanced data processing tools / é«˜çº§æ•°æ®å¤„ç†å·¥å…·\n",
    "- **Feature Engineering**: Comprehensive feature creation / ç»¼åˆç‰¹å¾åˆ›å»º\n",
    "\n",
    "#### Part 2: Advanced Features / é«˜çº§åŠŸèƒ½\n",
    "- **Multi-Market Support**: Cross-market trading framework / è·¨å¸‚åœºäº¤æ˜“æ¡†æ¶\n",
    "- **Online Learning**: Incremental model updates / å¢é‡æ¨¡å‹æ›´æ–°\n",
    "- **Meta-Learning**: Learning to learn strategies / å­¦ä¹ ç­–ç•¥çš„ç­–ç•¥\n",
    "- **AutoML Integration**: Automated model selection / è‡ªåŠ¨æ¨¡å‹é€‰æ‹©\n",
    "\n",
    "#### Part 3: Production Tools / ç”Ÿäº§å·¥å…·\n",
    "- **Real-time Trading**: Live trading system / å®ç›˜äº¤æ˜“ç³»ç»Ÿ\n",
    "- **Monitoring & Alerting**: Comprehensive monitoring / ç»¼åˆç›‘æ§\n",
    "- **Debugging & Profiling**: Performance optimization / æ€§èƒ½ä¼˜åŒ–\n",
    "- **Deployment Tools**: Production deployment / ç”Ÿäº§éƒ¨ç½²\n",
    "\n",
    "#### Part 4: Best Practices / æœ€ä½³å®è·µ\n",
    "- **Code Organization**: Project structure / é¡¹ç›®ç»“æ„\n",
    "- **Performance Optimization**: Speed and memory / é€Ÿåº¦å’Œå†…å­˜\n",
    "- **Common Pitfalls**: Solutions to frequent issues / å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ\n",
    "- **Tips and Tricks**: Pro techniques / ä¸“ä¸šæŠ€å·§\n",
    "\n",
    "### Key Takeaways / å…³é”®è¦ç‚¹\n",
    "\n",
    "1. **Automation is Key**: Automate repetitive tasks / è‡ªåŠ¨åŒ–é‡å¤ä»»åŠ¡\n",
    "2. **Monitor Everything**: Track all metrics / è·Ÿè¸ªæ‰€æœ‰æŒ‡æ ‡\n",
    "3. **Handle Edge Cases**: Robust error handling / ç¨³å¥çš„é”™è¯¯å¤„ç†\n",
    "4. **Optimize Performance**: Profile and optimize / åˆ†æå’Œä¼˜åŒ–\n",
    "5. **Think Production**: Design for deployment / ä¸ºéƒ¨ç½²è€Œè®¾è®¡\n",
    "\n",
    "### Resources / èµ„æº\n",
    "\n",
    "- [Qlib Documentation](https://qlib.readthedocs.io/)\n",
    "- [Qlib GitHub](https://github.com/microsoft/qlib)\n",
    "- [Qlib Examples](https://github.com/microsoft/qlib/tree/main/examples)\n",
    "- [Community Forum](https://github.com/microsoft/qlib/discussions)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed the comprehensive Qlib tutorial series!\n",
    "\n",
    "**æ­å–œï¼** æ‚¨å·²å®Œæˆå…¨é¢çš„Qlibæ•™ç¨‹ç³»åˆ—ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}