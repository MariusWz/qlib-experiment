{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Utilities, Helpers and Advanced Features\n",
    "# 第五章：工具函数、辅助功能与高级特性\n",
    "\n",
    "**Author**: Microsoft Qlib Team  \n",
    "**License**: MIT License  \n",
    "**Last Updated**: 2025-01-09\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Table of Contents / 目录\n",
    "\n",
    "### Part 1: Core Utilities / 核心工具\n",
    "1. [Workflow Automation / 工作流自动化](#workflow-automation)\n",
    "2. [Experiment Management / 实验管理](#experiment-management)\n",
    "3. [Data Utilities / 数据工具](#data-utilities)\n",
    "4. [Feature Engineering Helpers / 特征工程辅助](#feature-engineering)\n",
    "\n",
    "### Part 2: Advanced Features / 高级功能\n",
    "5. [Multi-Market Support / 多市场支持](#multi-market)\n",
    "6. [Online Learning / 在线学习](#online-learning)\n",
    "7. [Meta-Learning / 元学习](#meta-learning)\n",
    "8. [AutoML Integration / 自动机器学习集成](#automl)\n",
    "\n",
    "### Part 3: Production Tools / 生产工具\n",
    "9. [Real-time Trading / 实时交易](#realtime-trading)\n",
    "10. [Monitoring and Alerting / 监控与告警](#monitoring)\n",
    "11. [Debugging and Profiling / 调试与性能分析](#debugging)\n",
    "12. [Deployment Tools / 部署工具](#deployment)\n",
    "\n",
    "### Part 4: Best Practices / 最佳实践\n",
    "13. [Code Organization / 代码组织](#code-organization)\n",
    "14. [Performance Optimization / 性能优化](#performance-optimization)\n",
    "15. [Common Pitfalls and Solutions / 常见问题与解决方案](#pitfalls)\n",
    "16. [Tips and Tricks / 技巧与窍门](#tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports / 设置和导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports / 必要导入\n",
    "import qlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "import yaml\n",
    "import logging\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "import concurrent.futures\n",
    "from functools import wraps, lru_cache\n",
    "import traceback\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Qlib imports / Qlib导入\n",
    "from qlib.data import D\n",
    "from qlib.config import C\n",
    "from qlib.workflow import R\n",
    "from qlib.utils import init_instance_by_config, flatten_dict\n",
    "from qlib.log import get_module_logger\n",
    "\n",
    "# Initialize Qlib / 初始化Qlib\n",
    "qlib.init()\n",
    "\n",
    "# Setup logging / 设置日志\n",
    "logger = get_module_logger(\"UtilsHelpers\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "print(\"✅ Environment initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Core Utilities / 核心工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Workflow Automation / 工作流自动化 <a id='workflow-automation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow automation utilities / 工作流自动化工具\n",
    "\n",
    "class WorkflowAutomation:\n",
    "    \"\"\"Complete workflow automation framework\n",
    "    完整的工作流自动化框架\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str = None):\n",
    "        self.config = self.load_config(config_path) if config_path else {}\n",
    "        self.pipeline_steps = []\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_config(self, config_path: str) -> dict:\n",
    "        \"\"\"Load workflow configuration / 加载工作流配置\"\"\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            if config_path.endswith('.yaml'):\n",
    "                return yaml.safe_load(f)\n",
    "            elif config_path.endswith('.json'):\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def add_step(self, name: str, func, **kwargs):\n",
    "        \"\"\"Add a step to the pipeline / 添加管道步骤\"\"\"\n",
    "        self.pipeline_steps.append({\n",
    "            'name': name,\n",
    "            'func': func,\n",
    "            'kwargs': kwargs\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def run(self, parallel: bool = False):\n",
    "        \"\"\"Execute the workflow / 执行工作流\"\"\"\n",
    "        print(f\"Starting workflow with {len(self.pipeline_steps)} steps...\")\n",
    "        \n",
    "        if parallel:\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                futures = []\n",
    "                for step in self.pipeline_steps:\n",
    "                    future = executor.submit(self._execute_step, step)\n",
    "                    futures.append((step['name'], future))\n",
    "                \n",
    "                for name, future in futures:\n",
    "                    self.results[name] = future.result()\n",
    "        else:\n",
    "            for step in self.pipeline_steps:\n",
    "                self.results[step['name']] = self._execute_step(step)\n",
    "        \n",
    "        print(\"✅ Workflow completed\")\n",
    "        return self.results\n",
    "    \n",
    "    def _execute_step(self, step: dict):\n",
    "        \"\"\"Execute a single step / 执行单个步骤\"\"\"\n",
    "        print(f\"  Executing: {step['name']}...\")\n",
    "        try:\n",
    "            result = step['func'](**step['kwargs'])\n",
    "            print(f\"    ✅ {step['name']} completed\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"    ❌ {step['name']} failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Example workflow / 示例工作流\n",
    "def data_preparation(market='csi300'):\n",
    "    \"\"\"Prepare data / 准备数据\"\"\"\n",
    "    return f\"Data prepared for {market}\"\n",
    "\n",
    "def model_training(data=None):\n",
    "    \"\"\"Train model / 训练模型\"\"\"\n",
    "    return \"Model trained\"\n",
    "\n",
    "def backtesting(model=None):\n",
    "    \"\"\"Run backtest / 运行回测\"\"\"\n",
    "    return \"Backtest completed\"\n",
    "\n",
    "# Create and run workflow / 创建并运行工作流\n",
    "workflow = WorkflowAutomation()\n",
    "workflow.add_step('data_prep', data_preparation, market='csi300')\n",
    "workflow.add_step('training', model_training)\n",
    "workflow.add_step('backtest', backtesting)\n",
    "\n",
    "results = workflow.run(parallel=False)\n",
    "print(f\"\\nResults: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline decorator for automatic workflow creation / 用于自动创建工作流的管道装饰器\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"Pipeline decorator for workflow automation\n",
    "    用于工作流自动化的管道装饰器\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.steps = []\n",
    "        self.cache = {}\n",
    "    \n",
    "    def step(self, name: str, cache: bool = False):\n",
    "        \"\"\"Decorator to mark a function as a pipeline step\n",
    "        将函数标记为管道步骤的装饰器\n",
    "        \"\"\"\n",
    "        def decorator(func):\n",
    "            @wraps(func)\n",
    "            def wrapper(*args, **kwargs):\n",
    "                # Check cache / 检查缓存\n",
    "                if cache and name in self.cache:\n",
    "                    print(f\"  Using cached result for {name}\")\n",
    "                    return self.cache[name]\n",
    "                \n",
    "                # Execute function / 执行函数\n",
    "                print(f\"  Executing {name}...\")\n",
    "                start_time = time.time()\n",
    "                result = func(*args, **kwargs)\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                # Cache result if requested / 如果需要则缓存结果\n",
    "                if cache:\n",
    "                    self.cache[name] = result\n",
    "                \n",
    "                print(f\"    ✅ {name} completed in {elapsed:.2f}s\")\n",
    "                return result\n",
    "            \n",
    "            # Register step / 注册步骤\n",
    "            self.steps.append((name, wrapper))\n",
    "            return wrapper\n",
    "        return decorator\n",
    "    \n",
    "    def run_all(self, context: dict = None):\n",
    "        \"\"\"Run all registered steps / 运行所有注册的步骤\"\"\"\n",
    "        context = context or {}\n",
    "        print(f\"Running pipeline with {len(self.steps)} steps...\")\n",
    "        \n",
    "        for name, func in self.steps:\n",
    "            try:\n",
    "                context[name] = func(context)\n",
    "            except Exception as e:\n",
    "                print(f\"    ❌ Error in {name}: {str(e)}\")\n",
    "                context[name] = None\n",
    "        \n",
    "        return context\n",
    "\n",
    "# Example usage / 使用示例\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.step(\"load_data\", cache=True)\n",
    "def load_data(context):\n",
    "    \"\"\"Load market data / 加载市场数据\"\"\"\n",
    "    # Simulate data loading\n",
    "    return pd.DataFrame(np.random.randn(100, 5), columns=['A', 'B', 'C', 'D', 'E'])\n",
    "\n",
    "@pipeline.step(\"preprocess\", cache=False)\n",
    "def preprocess(context):\n",
    "    \"\"\"Preprocess data / 预处理数据\"\"\"\n",
    "    data = context.get('load_data')\n",
    "    if data is not None:\n",
    "        return data.fillna(0)\n",
    "    return None\n",
    "\n",
    "@pipeline.step(\"analyze\")\n",
    "def analyze(context):\n",
    "    \"\"\"Analyze data / 分析数据\"\"\"\n",
    "    data = context.get('preprocess')\n",
    "    if data is not None:\n",
    "        return data.describe()\n",
    "    return None\n",
    "\n",
    "# Run pipeline / 运行管道\n",
    "results = pipeline.run_all()\n",
    "print(f\"\\nPipeline completed with {len(results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment Management / 实验管理 <a id='experiment-management'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced experiment management / 高级实验管理\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Experiment configuration / 实验配置\"\"\"\n",
    "    name: str\n",
    "    model_type: str\n",
    "    dataset: str\n",
    "    hyperparameters: dict\n",
    "    metrics: List[str]\n",
    "    tags: List[str] = None\n",
    "    description: str = \"\"\n",
    "\n",
    "class ExperimentManager:\n",
    "    \"\"\"Comprehensive experiment management system\n",
    "    综合实验管理系统\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: str = \"./experiments\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "        self.experiments = {}\n",
    "        self.active_experiment = None\n",
    "        \n",
    "    def create_experiment(self, config: ExperimentConfig) -> str:\n",
    "        \"\"\"Create a new experiment / 创建新实验\"\"\"\n",
    "        exp_id = f\"{config.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        exp_dir = self.base_dir / exp_id\n",
    "        exp_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save configuration / 保存配置\n",
    "        config_path = exp_dir / \"config.json\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config.__dict__, f, indent=2)\n",
    "        \n",
    "        # Initialize experiment tracking / 初始化实验跟踪\n",
    "        self.experiments[exp_id] = {\n",
    "            'config': config,\n",
    "            'dir': exp_dir,\n",
    "            'status': 'created',\n",
    "            'created_at': datetime.now(),\n",
    "            'metrics': {},\n",
    "            'artifacts': []\n",
    "        }\n",
    "        \n",
    "        self.active_experiment = exp_id\n",
    "        print(f\"✅ Created experiment: {exp_id}\")\n",
    "        return exp_id\n",
    "    \n",
    "    def log_metric(self, name: str, value: float, step: int = None):\n",
    "        \"\"\"Log a metric / 记录指标\"\"\"\n",
    "        if self.active_experiment is None:\n",
    "            raise ValueError(\"No active experiment\")\n",
    "        \n",
    "        exp = self.experiments[self.active_experiment]\n",
    "        if name not in exp['metrics']:\n",
    "            exp['metrics'][name] = []\n",
    "        \n",
    "        exp['metrics'][name].append({\n",
    "            'value': value,\n",
    "            'step': step or len(exp['metrics'][name]),\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "    \n",
    "    def log_artifact(self, artifact_path: str, artifact_type: str = 'file'):\n",
    "        \"\"\"Log an artifact / 记录工件\"\"\"\n",
    "        if self.active_experiment is None:\n",
    "            raise ValueError(\"No active experiment\")\n",
    "        \n",
    "        exp = self.experiments[self.active_experiment]\n",
    "        exp['artifacts'].append({\n",
    "            'path': artifact_path,\n",
    "            'type': artifact_type,\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "    \n",
    "    def compare_experiments(self, exp_ids: List[str] = None):\n",
    "        \"\"\"Compare multiple experiments / 比较多个实验\"\"\"\n",
    "        if exp_ids is None:\n",
    "            exp_ids = list(self.experiments.keys())\n",
    "        \n",
    "        comparison = pd.DataFrame()\n",
    "        \n",
    "        for exp_id in exp_ids:\n",
    "            if exp_id in self.experiments:\n",
    "                exp = self.experiments[exp_id]\n",
    "                row = {\n",
    "                    'experiment': exp_id,\n",
    "                    'model': exp['config'].model_type,\n",
    "                    'dataset': exp['config'].dataset,\n",
    "                    'status': exp['status']\n",
    "                }\n",
    "                \n",
    "                # Add final metrics / 添加最终指标\n",
    "                for metric_name, values in exp['metrics'].items():\n",
    "                    if values:\n",
    "                        row[f'{metric_name}_final'] = values[-1]['value']\n",
    "                \n",
    "                comparison = pd.concat([comparison, pd.DataFrame([row])], ignore_index=True)\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def get_best_experiment(self, metric: str, mode: str = 'max'):\n",
    "        \"\"\"Get the best experiment based on a metric\n",
    "        基于指标获取最佳实验\n",
    "        \"\"\"\n",
    "        best_exp = None\n",
    "        best_value = None\n",
    "        \n",
    "        for exp_id, exp in self.experiments.items():\n",
    "            if metric in exp['metrics'] and exp['metrics'][metric]:\n",
    "                final_value = exp['metrics'][metric][-1]['value']\n",
    "                \n",
    "                if best_value is None:\n",
    "                    best_value = final_value\n",
    "                    best_exp = exp_id\n",
    "                elif mode == 'max' and final_value > best_value:\n",
    "                    best_value = final_value\n",
    "                    best_exp = exp_id\n",
    "                elif mode == 'min' and final_value < best_value:\n",
    "                    best_value = final_value\n",
    "                    best_exp = exp_id\n",
    "        \n",
    "        return best_exp, best_value\n",
    "\n",
    "# Example usage / 使用示例\n",
    "exp_manager = ExperimentManager()\n",
    "\n",
    "# Create experiments / 创建实验\n",
    "for i in range(3):\n",
    "    config = ExperimentConfig(\n",
    "        name=f\"test_exp_{i}\",\n",
    "        model_type=\"LightGBM\",\n",
    "        dataset=\"csi300\",\n",
    "        hyperparameters={'learning_rate': 0.1 * (i + 1)},\n",
    "        metrics=['sharpe', 'return'],\n",
    "        tags=['test', 'demo']\n",
    "    )\n",
    "    \n",
    "    exp_id = exp_manager.create_experiment(config)\n",
    "    \n",
    "    # Log metrics / 记录指标\n",
    "    exp_manager.log_metric('sharpe', np.random.random() + i * 0.1)\n",
    "    exp_manager.log_metric('return', np.random.random() * 0.2)\n",
    "\n",
    "# Compare experiments / 比较实验\n",
    "comparison = exp_manager.compare_experiments()\n",
    "print(\"\\nExperiment Comparison:\")\n",
    "print(comparison)\n",
    "\n",
    "# Find best experiment / 找到最佳实验\n",
    "best_exp, best_sharpe = exp_manager.get_best_experiment('sharpe', mode='max')\n",
    "print(f\"\\nBest experiment: {best_exp} with Sharpe: {best_sharpe:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Utilities / 数据工具 <a id='data-utilities'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced data utilities / 高级数据工具\n",
    "\n",
    "class DataUtils:\n",
    "    \"\"\"Comprehensive data utility functions\n",
    "    综合数据工具函数\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def resample_data(data: pd.DataFrame, source_freq: str = 'D', \n",
    "                     target_freq: str = 'W') -> pd.DataFrame:\n",
    "        \"\"\"Resample time series data / 重采样时间序列数据\"\"\"\n",
    "        if isinstance(data.index, pd.MultiIndex):\n",
    "            # Handle multi-index (datetime, instrument)\n",
    "            return data.groupby(level='instrument').resample(\n",
    "                target_freq, level='datetime'\n",
    "            ).agg({\n",
    "                col: 'last' if 'price' in col.lower() else 'sum' \n",
    "                for col in data.columns\n",
    "            })\n",
    "        else:\n",
    "            return data.resample(target_freq).last()\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_outliers(data: pd.DataFrame, method: str = 'iqr', \n",
    "                       threshold: float = 1.5) -> pd.DataFrame:\n",
    "        \"\"\"Detect outliers in data / 检测数据中的异常值\"\"\"\n",
    "        outliers = pd.DataFrame(False, index=data.index, columns=data.columns)\n",
    "        \n",
    "        if method == 'iqr':\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = (data < (Q1 - threshold * IQR)) | (data > (Q3 + threshold * IQR))\n",
    "        elif method == 'zscore':\n",
    "            z_scores = np.abs((data - data.mean()) / data.std())\n",
    "            outliers = z_scores > threshold\n",
    "        elif method == 'isolation_forest':\n",
    "            from sklearn.ensemble import IsolationForest\n",
    "            clf = IsolationForest(contamination=0.1, random_state=42)\n",
    "            outliers_pred = clf.fit_predict(data)\n",
    "            outliers = pd.DataFrame(\n",
    "                outliers_pred == -1, \n",
    "                index=data.index, \n",
    "                columns=['outlier']\n",
    "            )\n",
    "        \n",
    "        return outliers\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_lag_features(data: pd.DataFrame, lags: List[int], \n",
    "                          columns: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Create lag features / 创建滞后特征\"\"\"\n",
    "        if columns is None:\n",
    "            columns = data.columns.tolist()\n",
    "        \n",
    "        lag_features = data.copy()\n",
    "        \n",
    "        for col in columns:\n",
    "            for lag in lags:\n",
    "                lag_features[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
    "        \n",
    "        return lag_features\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_rolling_features(data: pd.DataFrame, windows: List[int],\n",
    "                                 functions: List[str] = ['mean', 'std']) -> pd.DataFrame:\n",
    "        \"\"\"Calculate rolling window features / 计算滚动窗口特征\"\"\"\n",
    "        rolling_features = data.copy()\n",
    "        \n",
    "        for col in data.columns:\n",
    "            for window in windows:\n",
    "                for func in functions:\n",
    "                    feature_name = f'{col}_roll_{window}_{func}'\n",
    "                    if func == 'mean':\n",
    "                        rolling_features[feature_name] = data[col].rolling(window).mean()\n",
    "                    elif func == 'std':\n",
    "                        rolling_features[feature_name] = data[col].rolling(window).std()\n",
    "                    elif func == 'max':\n",
    "                        rolling_features[feature_name] = data[col].rolling(window).max()\n",
    "                    elif func == 'min':\n",
    "                        rolling_features[feature_name] = data[col].rolling(window).min()\n",
    "        \n",
    "        return rolling_features\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=128)\n",
    "    def get_trading_calendar(market: str, start: str, end: str) -> pd.DatetimeIndex:\n",
    "        \"\"\"Get trading calendar with caching / 获取交易日历（带缓存）\"\"\"\n",
    "        return D.calendar(start_time=start, end_time=end, freq='day')\n",
    "\n",
    "# Example usage / 使用示例\n",
    "data_utils = DataUtils()\n",
    "\n",
    "# Create sample data / 创建示例数据\n",
    "dates = pd.date_range('2024-01-01', '2024-01-31', freq='D')\n",
    "sample_data = pd.DataFrame({\n",
    "    'price': 100 + np.random.randn(len(dates)).cumsum(),\n",
    "    'volume': np.random.randint(1000, 10000, len(dates)),\n",
    "    'returns': np.random.randn(len(dates)) * 0.02\n",
    "}, index=dates)\n",
    "\n",
    "# Detect outliers / 检测异常值\n",
    "outliers = data_utils.detect_outliers(sample_data, method='zscore', threshold=2)\n",
    "print(f\"Outliers detected: {outliers.sum().sum()}\")\n",
    "\n",
    "# Create lag features / 创建滞后特征\n",
    "lag_data = data_utils.create_lag_features(sample_data, lags=[1, 5, 10], columns=['returns'])\n",
    "print(f\"\\nColumns after lag features: {lag_data.columns.tolist()}\")\n",
    "\n",
    "# Calculate rolling features / 计算滚动特征\n",
    "rolling_data = data_utils.calculate_rolling_features(\n",
    "    sample_data[['price', 'returns']], \n",
    "    windows=[5, 20],\n",
    "    functions=['mean', 'std']\n",
    ")\n",
    "print(f\"\\nColumns after rolling features: {rolling_data.columns.tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering Helpers / 特征工程辅助 <a id='feature-engineering'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced feature engineering helpers / 高级特征工程辅助\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Advanced feature engineering toolkit\n",
    "    高级特征工程工具包\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_importance = {}\n",
    "        self.feature_stats = {}\n",
    "    \n",
    "    def create_technical_indicators(self, ohlcv: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create comprehensive technical indicators\n",
    "        创建综合技术指标\n",
    "        \"\"\"\n",
    "        features = ohlcv.copy()\n",
    "        \n",
    "        # Price-based indicators / 基于价格的指标\n",
    "        features['SMA_5'] = ohlcv['close'].rolling(5).mean()\n",
    "        features['SMA_20'] = ohlcv['close'].rolling(20).mean()\n",
    "        features['EMA_12'] = ohlcv['close'].ewm(span=12).mean()\n",
    "        features['EMA_26'] = ohlcv['close'].ewm(span=26).mean()\n",
    "        \n",
    "        # MACD\n",
    "        features['MACD'] = features['EMA_12'] - features['EMA_26']\n",
    "        features['MACD_signal'] = features['MACD'].ewm(span=9).mean()\n",
    "        features['MACD_hist'] = features['MACD'] - features['MACD_signal']\n",
    "        \n",
    "        # Bollinger Bands / 布林带\n",
    "        bb_period = 20\n",
    "        bb_std = ohlcv['close'].rolling(bb_period).std()\n",
    "        bb_mean = ohlcv['close'].rolling(bb_period).mean()\n",
    "        features['BB_upper'] = bb_mean + 2 * bb_std\n",
    "        features['BB_lower'] = bb_mean - 2 * bb_std\n",
    "        features['BB_width'] = features['BB_upper'] - features['BB_lower']\n",
    "        features['BB_position'] = (ohlcv['close'] - features['BB_lower']) / features['BB_width']\n",
    "        \n",
    "        # RSI\n",
    "        delta = ohlcv['close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "        rs = gain / loss\n",
    "        features['RSI'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Volume indicators / 成交量指标\n",
    "        features['volume_ratio'] = ohlcv['volume'] / ohlcv['volume'].rolling(20).mean()\n",
    "        features['OBV'] = (np.sign(ohlcv['close'].diff()) * ohlcv['volume']).cumsum()\n",
    "        \n",
    "        # Volatility / 波动率\n",
    "        features['volatility_20'] = ohlcv['close'].pct_change().rolling(20).std()\n",
    "        features['ATR'] = self._calculate_atr(ohlcv)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _calculate_atr(self, ohlcv: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "        \"\"\"Calculate Average True Range / 计算平均真实范围\"\"\"\n",
    "        high_low = ohlcv['high'] - ohlcv['low']\n",
    "        high_close = np.abs(ohlcv['high'] - ohlcv['close'].shift())\n",
    "        low_close = np.abs(ohlcv['low'] - ohlcv['close'].shift())\n",
    "        \n",
    "        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "        return true_range.rolling(period).mean()\n",
    "    \n",
    "    def create_interaction_features(self, data: pd.DataFrame, \n",
    "                                  max_degree: int = 2) -> pd.DataFrame:\n",
    "        \"\"\"Create polynomial and interaction features\n",
    "        创建多项式和交互特征\n",
    "        \"\"\"\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        \n",
    "        poly = PolynomialFeatures(degree=max_degree, include_bias=False)\n",
    "        poly_features = poly.fit_transform(data)\n",
    "        \n",
    "        feature_names = poly.get_feature_names_out(data.columns)\n",
    "        return pd.DataFrame(poly_features, columns=feature_names, index=data.index)\n",
    "    \n",
    "    def select_features(self, X: pd.DataFrame, y: pd.Series, \n",
    "                       method: str = 'mutual_info', k: int = 20) -> List[str]:\n",
    "        \"\"\"Feature selection using various methods\n",
    "        使用各种方法进行特征选择\n",
    "        \"\"\"\n",
    "        from sklearn.feature_selection import (\n",
    "            SelectKBest, mutual_info_regression, f_regression\n",
    "        )\n",
    "        \n",
    "        if method == 'mutual_info':\n",
    "            selector = SelectKBest(mutual_info_regression, k=k)\n",
    "        elif method == 'f_regression':\n",
    "            selector = SelectKBest(f_regression, k=k)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        selector.fit(X, y)\n",
    "        selected_features = X.columns[selector.get_support()].tolist()\n",
    "        \n",
    "        # Store feature importance / 存储特征重要性\n",
    "        self.feature_importance[method] = dict(zip(X.columns, selector.scores_))\n",
    "        \n",
    "        return selected_features\n",
    "    \n",
    "    def create_target_encoding(self, data: pd.DataFrame, \n",
    "                              categorical_cols: List[str],\n",
    "                              target_col: str) -> pd.DataFrame:\n",
    "        \"\"\"Target encoding for categorical features\n",
    "        分类特征的目标编码\n",
    "        \"\"\"\n",
    "        encoded_data = data.copy()\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            # Calculate mean target for each category\n",
    "            target_mean = data.groupby(col)[target_col].mean()\n",
    "            \n",
    "            # Add noise to prevent overfitting\n",
    "            noise_level = 0.01\n",
    "            target_mean += np.random.normal(0, noise_level, len(target_mean))\n",
    "            \n",
    "            # Map to original data\n",
    "            encoded_data[f'{col}_target_enc'] = data[col].map(target_mean)\n",
    "        \n",
    "        return encoded_data\n",
    "\n",
    "# Example usage / 使用示例\n",
    "fe = FeatureEngineer()\n",
    "\n",
    "# Create sample OHLCV data / 创建示例OHLCV数据\n",
    "dates = pd.date_range('2024-01-01', '2024-03-31', freq='D')\n",
    "ohlcv = pd.DataFrame({\n",
    "    'open': 100 + np.random.randn(len(dates)).cumsum(),\n",
    "    'high': 102 + np.random.randn(len(dates)).cumsum(),\n",
    "    'low': 98 + np.random.randn(len(dates)).cumsum(),\n",
    "    'close': 100 + np.random.randn(len(dates)).cumsum(),\n",
    "    'volume': np.random.randint(1000000, 5000000, len(dates))\n",
    "}, index=dates)\n",
    "\n",
    "# Create technical indicators / 创建技术指标\n",
    "tech_features = fe.create_technical_indicators(ohlcv)\n",
    "print(f\"Technical features created: {len(tech_features.columns)} columns\")\n",
    "print(f\"Sample features: {tech_features.columns.tolist()[:10]}...\")\n",
    "\n",
    "# Feature selection example / 特征选择示例\n",
    "X = tech_features.dropna()\n",
    "y = X['close'].pct_change().shift(-1).fillna(0)  # Next day return as target\n",
    "selected = fe.select_features(X, y, method='f_regression', k=10)\n",
    "print(f\"\\nTop 10 selected features: {selected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Advanced Features / 高级功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Market Support / 多市场支持 <a id='multi-market'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-market trading framework / 多市场交易框架\n",
    "\n",
    "class MultiMarketFramework:\n",
    "    \"\"\"Framework for multi-market trading\n",
    "    多市场交易框架\n",
    "    \"\"\"\n",
    "    \n",
    "    # Market configurations / 市场配置\n",
    "    MARKET_CONFIG = {\n",
    "        'CN': {\n",
    "            'name': 'China A-Share',\n",
    "            'trading_hours': '09:30-15:00',\n",
    "            'timezone': 'Asia/Shanghai',\n",
    "            'currency': 'CNY',\n",
    "            'indices': ['csi300', 'csi500', 'csi1000'],\n",
    "            'limit': 0.10,  # 10% price limit\n",
    "        },\n",
    "        'US': {\n",
    "            'name': 'US Market',\n",
    "            'trading_hours': '09:30-16:00',\n",
    "            'timezone': 'America/New_York',\n",
    "            'currency': 'USD',\n",
    "            'indices': ['sp500', 'nasdaq100', 'russell2000'],\n",
    "            'limit': None,  # No price limit\n",
    "        },\n",
    "        'HK': {\n",
    "            'name': 'Hong Kong Market',\n",
    "            'trading_hours': '09:30-16:00',\n",
    "            'timezone': 'Asia/Hong_Kong',\n",
    "            'currency': 'HKD',\n",
    "            'indices': ['hsi', 'hscei'],\n",
    "            'limit': None,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, markets: List[str]):\n",
    "        self.markets = markets\n",
    "        self.market_data = {}\n",
    "        self.portfolios = {}\n",
    "        \n",
    "    def initialize_markets(self):\n",
    "        \"\"\"Initialize multiple markets / 初始化多个市场\"\"\"\n",
    "        for market in self.markets:\n",
    "            if market in self.MARKET_CONFIG:\n",
    "                config = self.MARKET_CONFIG[market]\n",
    "                print(f\"Initializing {config['name']}...\")\n",
    "                \n",
    "                # Initialize market-specific components\n",
    "                self.portfolios[market] = {\n",
    "                    'positions': {},\n",
    "                    'cash': 1000000,  # Initial cash\n",
    "                    'currency': config['currency']\n",
    "                }\n",
    "    \n",
    "    def convert_currency(self, amount: float, from_currency: str, \n",
    "                        to_currency: str) -> float:\n",
    "        \"\"\"Currency conversion / 货币转换\"\"\"\n",
    "        # Simplified exchange rates / 简化的汇率\n",
    "        exchange_rates = {\n",
    "            'CNY_USD': 0.14,\n",
    "            'USD_CNY': 7.2,\n",
    "            'HKD_USD': 0.13,\n",
    "            'USD_HKD': 7.8,\n",
    "            'CNY_HKD': 1.1,\n",
    "            'HKD_CNY': 0.92\n",
    "        }\n",
    "        \n",
    "        if from_currency == to_currency:\n",
    "            return amount\n",
    "        \n",
    "        rate_key = f\"{from_currency}_{to_currency}\"\n",
    "        if rate_key in exchange_rates:\n",
    "            return amount * exchange_rates[rate_key]\n",
    "        \n",
    "        return amount\n",
    "    \n",
    "    def aggregate_portfolio(self, base_currency: str = 'USD') -> dict:\n",
    "        \"\"\"Aggregate multi-market portfolio / 聚合多市场组合\"\"\"\n",
    "        total_value = 0\n",
    "        aggregated = {'positions': {}, 'cash': 0, 'currency': base_currency}\n",
    "        \n",
    "        for market, portfolio in self.portfolios.items():\n",
    "            market_config = self.MARKET_CONFIG[market]\n",
    "            \n",
    "            # Convert cash to base currency\n",
    "            cash_in_base = self.convert_currency(\n",
    "                portfolio['cash'],\n",
    "                market_config['currency'],\n",
    "                base_currency\n",
    "            )\n",
    "            aggregated['cash'] += cash_in_base\n",
    "            \n",
    "            # Aggregate positions\n",
    "            for stock, position in portfolio['positions'].items():\n",
    "                key = f\"{market}:{stock}\"\n",
    "                aggregated['positions'][key] = position\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def cross_market_arbitrage(self, symbol: str) -> dict:\n",
    "        \"\"\"Identify cross-market arbitrage opportunities\n",
    "        识别跨市场套利机会\n",
    "        \"\"\"\n",
    "        opportunities = []\n",
    "        \n",
    "        # Example: A+H share arbitrage\n",
    "        # Check if stock is listed in multiple markets\n",
    "        prices = {}\n",
    "        for market in self.markets:\n",
    "            # Simulate price fetching\n",
    "            prices[market] = np.random.uniform(90, 110)\n",
    "        \n",
    "        # Find arbitrage opportunities\n",
    "        for m1 in self.markets:\n",
    "            for m2 in self.markets:\n",
    "                if m1 != m2:\n",
    "                    price_diff = abs(prices[m1] - prices[m2]) / prices[m1]\n",
    "                    if price_diff > 0.02:  # 2% threshold\n",
    "                        opportunities.append({\n",
    "                            'symbol': symbol,\n",
    "                            'market_1': m1,\n",
    "                            'market_2': m2,\n",
    "                            'price_1': prices[m1],\n",
    "                            'price_2': prices[m2],\n",
    "                            'spread': price_diff\n",
    "                        })\n",
    "        \n",
    "        return opportunities\n",
    "\n",
    "# Example usage / 使用示例\n",
    "multi_market = MultiMarketFramework(['CN', 'US', 'HK'])\n",
    "multi_market.initialize_markets()\n",
    "\n",
    "# Aggregate portfolio / 聚合组合\n",
    "aggregated = multi_market.aggregate_portfolio(base_currency='USD')\n",
    "print(f\"\\nAggregated portfolio in USD:\")\n",
    "print(f\"  Total cash: ${aggregated['cash']:,.2f}\")\n",
    "\n",
    "# Check arbitrage opportunities / 检查套利机会\n",
    "arb_ops = multi_market.cross_market_arbitrage('BABA')\n",
    "if arb_ops:\n",
    "    print(f\"\\nArbitrage opportunities found:\")\n",
    "    for op in arb_ops:\n",
    "        print(f\"  {op['symbol']}: {op['market_1']} vs {op['market_2']}, spread: {op['spread']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Online Learning / 在线学习 <a id='online-learning'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online learning framework / 在线学习框架\n",
    "\n",
    "class OnlineLearningFramework:\n",
    "    \"\"\"Framework for online/incremental learning\n",
    "    在线/增量学习框架\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, update_frequency: int = 100):\n",
    "        self.base_model = base_model\n",
    "        self.update_frequency = update_frequency\n",
    "        self.data_buffer = []\n",
    "        self.performance_history = []\n",
    "        self.update_count = 0\n",
    "        \n",
    "    def update_model(self, new_data: pd.DataFrame, new_labels: pd.Series):\n",
    "        \"\"\"Incrementally update the model / 增量更新模型\"\"\"\n",
    "        # Add to buffer\n",
    "        self.data_buffer.append((new_data, new_labels))\n",
    "        \n",
    "        # Check if update is needed\n",
    "        if len(self.data_buffer) >= self.update_frequency:\n",
    "            print(f\"Updating model (update #{self.update_count + 1})...\")\n",
    "            \n",
    "            # Combine buffered data\n",
    "            X = pd.concat([d[0] for d in self.data_buffer])\n",
    "            y = pd.concat([d[1] for d in self.data_buffer])\n",
    "            \n",
    "            # Partial fit (for models that support it)\n",
    "            if hasattr(self.base_model, 'partial_fit'):\n",
    "                self.base_model.partial_fit(X, y)\n",
    "            else:\n",
    "                # Retrain on recent data\n",
    "                self.base_model.fit(X, y)\n",
    "            \n",
    "            # Clear buffer\n",
    "            self.data_buffer = []\n",
    "            self.update_count += 1\n",
    "            \n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def adaptive_learning_rate(self, performance_metric: float):\n",
    "        \"\"\"Adjust learning rate based on performance\n",
    "        基于性能调整学习率\n",
    "        \"\"\"\n",
    "        self.performance_history.append(performance_metric)\n",
    "        \n",
    "        if len(self.performance_history) > 10:\n",
    "            recent_performance = self.performance_history[-10:]\n",
    "            performance_trend = np.polyfit(range(10), recent_performance, 1)[0]\n",
    "            \n",
    "            # Adjust learning rate based on trend\n",
    "            if hasattr(self.base_model, 'learning_rate'):\n",
    "                if performance_trend < 0:  # Performance declining\n",
    "                    self.base_model.learning_rate *= 0.9\n",
    "                    print(f\"Decreased learning rate to {self.base_model.learning_rate:.4f}\")\n",
    "                elif performance_trend > 0.01:  # Performance improving\n",
    "                    self.base_model.learning_rate *= 1.1\n",
    "                    print(f\"Increased learning rate to {self.base_model.learning_rate:.4f}\")\n",
    "    \n",
    "    def concept_drift_detection(self, predictions: np.ndarray, \n",
    "                              actuals: np.ndarray,\n",
    "                              threshold: float = 0.1):\n",
    "        \"\"\"Detect concept drift in data / 检测数据中的概念漂移\"\"\"\n",
    "        error = np.abs(predictions - actuals).mean()\n",
    "        \n",
    "        if not hasattr(self, 'baseline_error'):\n",
    "            self.baseline_error = error\n",
    "            return False\n",
    "        \n",
    "        drift_ratio = (error - self.baseline_error) / self.baseline_error\n",
    "        \n",
    "        if drift_ratio > threshold:\n",
    "            print(f\"⚠️ Concept drift detected! Error increased by {drift_ratio:.2%}\")\n",
    "            return True\n",
    "        \n",
    "        # Update baseline with exponential moving average\n",
    "        self.baseline_error = 0.9 * self.baseline_error + 0.1 * error\n",
    "        return False\n",
    "\n",
    "# Example usage / 使用示例\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Create online learning model / 创建在线学习模型\n",
    "base_model = SGDRegressor(learning_rate='constant', eta0=0.01)\n",
    "online_learner = OnlineLearningFramework(base_model, update_frequency=50)\n",
    "\n",
    "# Simulate streaming data / 模拟流数据\n",
    "for i in range(200):\n",
    "    # Generate new data batch\n",
    "    X_new = pd.DataFrame(np.random.randn(10, 5), columns=[f'f{j}' for j in range(5)])\n",
    "    y_new = pd.Series(np.random.randn(10))\n",
    "    \n",
    "    # Update model\n",
    "    updated = online_learner.update_model(X_new, y_new)\n",
    "    \n",
    "    if updated:\n",
    "        # Make predictions and check for drift\n",
    "        predictions = base_model.predict(X_new)\n",
    "        drift = online_learner.concept_drift_detection(predictions, y_new.values)\n",
    "        \n",
    "        if drift:\n",
    "            print(\"  Triggering model retraining due to concept drift...\")\n",
    "\n",
    "print(f\"\\nTotal model updates: {online_learner.update_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Production Tools / 生产工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-time Trading Integration / 实时交易集成 <a id='realtime-trading'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time trading system / 实时交易系统\n",
    "\n",
    "class RealTimeTradingSystem:\n",
    "    \"\"\"Real-time trading system with risk management\n",
    "    带风险管理的实时交易系统\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, risk_manager=None):\n",
    "        self.model = model\n",
    "        self.risk_manager = risk_manager\n",
    "        self.order_queue = []\n",
    "        self.positions = {}\n",
    "        self.trading_status = 'STOPPED'\n",
    "        self.last_update = None\n",
    "        \n",
    "    def start_trading(self):\n",
    "        \"\"\"Start real-time trading / 开始实时交易\"\"\"\n",
    "        self.trading_status = 'RUNNING'\n",
    "        print(\"🟢 Trading system started\")\n",
    "        \n",
    "        # Start event loop (simplified)\n",
    "        self._trading_loop()\n",
    "    \n",
    "    def stop_trading(self):\n",
    "        \"\"\"Stop trading / 停止交易\"\"\"\n",
    "        self.trading_status = 'STOPPED'\n",
    "        print(\"🔴 Trading system stopped\")\n",
    "    \n",
    "    def _trading_loop(self):\n",
    "        \"\"\"Main trading loop / 主交易循环\"\"\"\n",
    "        import asyncio\n",
    "        \n",
    "        async def process_market_data():\n",
    "            while self.trading_status == 'RUNNING':\n",
    "                # Fetch real-time data (simulated)\n",
    "                market_data = self._fetch_market_data()\n",
    "                \n",
    "                # Generate signals\n",
    "                signals = self._generate_signals(market_data)\n",
    "                \n",
    "                # Risk check\n",
    "                if self.risk_manager:\n",
    "                    signals = self.risk_manager.check_signals(signals, self.positions)\n",
    "                \n",
    "                # Generate orders\n",
    "                orders = self._generate_orders(signals)\n",
    "                \n",
    "                # Execute orders\n",
    "                await self._execute_orders(orders)\n",
    "                \n",
    "                # Update positions\n",
    "                self._update_positions()\n",
    "                \n",
    "                await asyncio.sleep(1)  # Wait 1 second\n",
    "        \n",
    "        # Run async loop (simplified for demo)\n",
    "        print(\"Trading loop simulation...\")\n",
    "        for _ in range(3):  # Simulate 3 iterations\n",
    "            market_data = self._fetch_market_data()\n",
    "            signals = self._generate_signals(market_data)\n",
    "            print(f\"  Generated {len(signals)} signals\")\n",
    "            time.sleep(1)\n",
    "    \n",
    "    def _fetch_market_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Fetch real-time market data / 获取实时市场数据\"\"\"\n",
    "        # Simulated real-time data\n",
    "        stocks = ['AAPL', 'GOOGL', 'MSFT', 'AMZN']\n",
    "        data = pd.DataFrame({\n",
    "            'symbol': stocks,\n",
    "            'price': np.random.uniform(100, 200, len(stocks)),\n",
    "            'volume': np.random.randint(1000000, 10000000, len(stocks)),\n",
    "            'bid': np.random.uniform(99, 199, len(stocks)),\n",
    "            'ask': np.random.uniform(101, 201, len(stocks))\n",
    "        })\n",
    "        self.last_update = datetime.now()\n",
    "        return data\n",
    "    \n",
    "    def _generate_signals(self, market_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Generate trading signals / 生成交易信号\"\"\"\n",
    "        # Use model to generate signals\n",
    "        signals = pd.DataFrame({\n",
    "            'symbol': market_data['symbol'],\n",
    "            'signal': np.random.uniform(-1, 1, len(market_data)),\n",
    "            'confidence': np.random.uniform(0.5, 1.0, len(market_data))\n",
    "        })\n",
    "        return signals\n",
    "    \n",
    "    def _generate_orders(self, signals: pd.DataFrame) -> List[dict]:\n",
    "        \"\"\"Generate orders from signals / 从信号生成订单\"\"\"\n",
    "        orders = []\n",
    "        \n",
    "        for _, signal in signals.iterrows():\n",
    "            if abs(signal['signal']) > 0.5 and signal['confidence'] > 0.7:\n",
    "                order = {\n",
    "                    'symbol': signal['symbol'],\n",
    "                    'side': 'BUY' if signal['signal'] > 0 else 'SELL',\n",
    "                    'quantity': int(abs(signal['signal']) * 100),\n",
    "                    'order_type': 'MARKET',\n",
    "                    'timestamp': datetime.now()\n",
    "                }\n",
    "                orders.append(order)\n",
    "        \n",
    "        return orders\n",
    "    \n",
    "    async def _execute_orders(self, orders: List[dict]):\n",
    "        \"\"\"Execute orders / 执行订单\"\"\"\n",
    "        for order in orders:\n",
    "            print(f\"  Executing {order['side']} {order['quantity']} {order['symbol']}\")\n",
    "            self.order_queue.append(order)\n",
    "    \n",
    "    def _update_positions(self):\n",
    "        \"\"\"Update positions / 更新持仓\"\"\"\n",
    "        for order in self.order_queue:\n",
    "            symbol = order['symbol']\n",
    "            quantity = order['quantity'] if order['side'] == 'BUY' else -order['quantity']\n",
    "            \n",
    "            if symbol in self.positions:\n",
    "                self.positions[symbol] += quantity\n",
    "            else:\n",
    "                self.positions[symbol] = quantity\n",
    "        \n",
    "        self.order_queue = []\n",
    "\n",
    "class RiskManager:\n",
    "    \"\"\"Risk management system / 风险管理系统\"\"\"\n",
    "    \n",
    "    def __init__(self, max_position_size: float = 0.1, \n",
    "                 max_total_exposure: float = 0.8):\n",
    "        self.max_position_size = max_position_size\n",
    "        self.max_total_exposure = max_total_exposure\n",
    "        \n",
    "    def check_signals(self, signals: pd.DataFrame, \n",
    "                     current_positions: dict) -> pd.DataFrame:\n",
    "        \"\"\"Check signals against risk limits / 根据风险限制检查信号\"\"\"\n",
    "        # Apply position size limits\n",
    "        signals['signal'] = signals['signal'].clip(-self.max_position_size, \n",
    "                                                   self.max_position_size)\n",
    "        \n",
    "        # Check total exposure\n",
    "        total_exposure = sum(abs(v) for v in current_positions.values())\n",
    "        if total_exposure > self.max_total_exposure:\n",
    "            print(\"⚠️ Risk limit reached, scaling down signals\")\n",
    "            signals['signal'] *= 0.5\n",
    "        \n",
    "        return signals\n",
    "\n",
    "# Example usage / 使用示例\n",
    "risk_mgr = RiskManager(max_position_size=0.1, max_total_exposure=0.8)\n",
    "trading_system = RealTimeTradingSystem(model=None, risk_manager=risk_mgr)\n",
    "\n",
    "# Start trading\n",
    "trading_system.start_trading()\n",
    "\n",
    "# Check positions\n",
    "print(f\"\\nCurrent positions: {trading_system.positions}\")\n",
    "print(f\"Last update: {trading_system.last_update}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Monitoring and Alerting / 监控与告警 <a id='monitoring'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring and alerting system / 监控与告警系统\n",
    "\n",
    "class MonitoringSystem:\n",
    "    \"\"\"Comprehensive monitoring and alerting system\n",
    "    综合监控与告警系统\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alert_config: dict = None):\n",
    "        self.metrics = {}\n",
    "        self.alerts = []\n",
    "        self.alert_config = alert_config or self._default_alert_config()\n",
    "        self.alert_handlers = []\n",
    "        \n",
    "    def _default_alert_config(self) -> dict:\n",
    "        \"\"\"Default alert configuration / 默认告警配置\"\"\"\n",
    "        return {\n",
    "            'max_drawdown': -0.10,\n",
    "            'daily_loss_limit': -0.05,\n",
    "            'position_limit': 0.20,\n",
    "            'volatility_threshold': 0.30,\n",
    "            'sharpe_threshold': 0.5\n",
    "        }\n",
    "    \n",
    "    def track_metric(self, name: str, value: float, timestamp: datetime = None):\n",
    "        \"\"\"Track a metric / 跟踪指标\"\"\"\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now()\n",
    "        \n",
    "        if name not in self.metrics:\n",
    "            self.metrics[name] = []\n",
    "        \n",
    "        self.metrics[name].append({'value': value, 'timestamp': timestamp})\n",
    "        \n",
    "        # Check for alerts\n",
    "        self._check_alerts(name, value)\n",
    "    \n",
    "    def _check_alerts(self, metric_name: str, value: float):\n",
    "        \"\"\"Check if alerts should be triggered / 检查是否应触发告警\"\"\"\n",
    "        alerts_triggered = []\n",
    "        \n",
    "        # Drawdown alert\n",
    "        if metric_name == 'drawdown' and value < self.alert_config['max_drawdown']:\n",
    "            alerts_triggered.append({\n",
    "                'type': 'CRITICAL',\n",
    "                'metric': metric_name,\n",
    "                'value': value,\n",
    "                'threshold': self.alert_config['max_drawdown'],\n",
    "                'message': f\"Drawdown {value:.2%} exceeds limit {self.alert_config['max_drawdown']:.2%}\"\n",
    "            })\n",
    "        \n",
    "        # Daily loss alert\n",
    "        if metric_name == 'daily_return' and value < self.alert_config['daily_loss_limit']:\n",
    "            alerts_triggered.append({\n",
    "                'type': 'WARNING',\n",
    "                'metric': metric_name,\n",
    "                'value': value,\n",
    "                'threshold': self.alert_config['daily_loss_limit'],\n",
    "                'message': f\"Daily loss {value:.2%} exceeds limit\"\n",
    "            })\n",
    "        \n",
    "        # Process alerts\n",
    "        for alert in alerts_triggered:\n",
    "            self._trigger_alert(alert)\n",
    "    \n",
    "    def _trigger_alert(self, alert: dict):\n",
    "        \"\"\"Trigger an alert / 触发告警\"\"\"\n",
    "        alert['timestamp'] = datetime.now()\n",
    "        self.alerts.append(alert)\n",
    "        \n",
    "        # Print alert\n",
    "        icon = '🔴' if alert['type'] == 'CRITICAL' else '⚠️'\n",
    "        print(f\"{icon} {alert['type']}: {alert['message']}\")\n",
    "        \n",
    "        # Call alert handlers\n",
    "        for handler in self.alert_handlers:\n",
    "            handler(alert)\n",
    "    \n",
    "    def add_alert_handler(self, handler):\n",
    "        \"\"\"Add custom alert handler / 添加自定义告警处理器\"\"\"\n",
    "        self.alert_handlers.append(handler)\n",
    "    \n",
    "    def generate_report(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate monitoring report / 生成监控报告\"\"\"\n",
    "        report_data = []\n",
    "        \n",
    "        for metric_name, values in self.metrics.items():\n",
    "            if values:\n",
    "                recent_values = [v['value'] for v in values[-100:]]  # Last 100\n",
    "                report_data.append({\n",
    "                    'Metric': metric_name,\n",
    "                    'Current': values[-1]['value'],\n",
    "                    'Mean': np.mean(recent_values),\n",
    "                    'Std': np.std(recent_values),\n",
    "                    'Min': np.min(recent_values),\n",
    "                    'Max': np.max(recent_values),\n",
    "                    'Count': len(values)\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(report_data)\n",
    "    \n",
    "    def plot_metrics(self, metrics: List[str] = None):\n",
    "        \"\"\"Plot monitored metrics / 绘制监控指标\"\"\"\n",
    "        if metrics is None:\n",
    "            metrics = list(self.metrics.keys())\n",
    "        \n",
    "        n_metrics = len(metrics)\n",
    "        if n_metrics == 0:\n",
    "            print(\"No metrics to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(n_metrics, 1, figsize=(12, 4*n_metrics))\n",
    "        if n_metrics == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, metric_name in zip(axes, metrics):\n",
    "            if metric_name in self.metrics:\n",
    "                values = self.metrics[metric_name]\n",
    "                timestamps = [v['timestamp'] for v in values]\n",
    "                metric_values = [v['value'] for v in values]\n",
    "                \n",
    "                ax.plot(timestamps, metric_values, linewidth=2)\n",
    "                ax.set_title(f'{metric_name} Monitoring')\n",
    "                ax.set_xlabel('Time')\n",
    "                ax.set_ylabel('Value')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add threshold lines if applicable\n",
    "                if metric_name == 'drawdown':\n",
    "                    ax.axhline(y=self.alert_config['max_drawdown'], \n",
    "                             color='red', linestyle='--', alpha=0.5,\n",
    "                             label='Alert Threshold')\n",
    "                    ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage / 使用示例\n",
    "monitor = MonitoringSystem()\n",
    "\n",
    "# Add custom alert handler / 添加自定义告警处理器\n",
    "def email_alert_handler(alert):\n",
    "    print(f\"  📧 Sending email alert: {alert['message']}\")\n",
    "\n",
    "monitor.add_alert_handler(email_alert_handler)\n",
    "\n",
    "# Simulate monitoring / 模拟监控\n",
    "for i in range(20):\n",
    "    # Track metrics\n",
    "    monitor.track_metric('drawdown', -np.random.uniform(0, 0.15))\n",
    "    monitor.track_metric('daily_return', np.random.uniform(-0.08, 0.08))\n",
    "    monitor.track_metric('sharpe_ratio', np.random.uniform(0.3, 1.5))\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Generate report / 生成报告\n",
    "report = monitor.generate_report()\n",
    "print(\"\\nMonitoring Report:\")\n",
    "print(report.round(4))\n",
    "\n",
    "# Plot metrics / 绘制指标\n",
    "monitor.plot_metrics(['drawdown', 'daily_return'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Best Practices / 最佳实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Common Pitfalls and Solutions / 常见问题与解决方案 <a id='pitfalls'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common pitfalls and solutions / 常见问题与解决方案\n",
    "\n",
    "class QlibBestPractices:\n",
    "    \"\"\"Collection of best practices and solutions\n",
    "    最佳实践和解决方案集合\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def avoid_look_ahead_bias(data: pd.DataFrame, shift_days: int = 1) -> pd.DataFrame:\n",
    "        \"\"\"Avoid look-ahead bias in features\n",
    "        避免特征中的未来函数\n",
    "        \"\"\"\n",
    "        # Shift all features to avoid using future information\n",
    "        feature_cols = [col for col in data.columns if col != 'label']\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            data[col] = data[col].shift(shift_days)\n",
    "        \n",
    "        return data.dropna()\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_missing_data(data: pd.DataFrame, method: str = 'forward_fill') -> pd.DataFrame:\n",
    "        \"\"\"Properly handle missing data\n",
    "        正确处理缺失数据\n",
    "        \"\"\"\n",
    "        if method == 'forward_fill':\n",
    "            # Forward fill then backward fill\n",
    "            data = data.fillna(method='ffill').fillna(method='bfill')\n",
    "        elif method == 'interpolate':\n",
    "            data = data.interpolate(method='linear')\n",
    "        elif method == 'drop':\n",
    "            data = data.dropna()\n",
    "        elif method == 'mean':\n",
    "            data = data.fillna(data.mean())\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_data_alignment(features: pd.DataFrame, \n",
    "                              labels: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Ensure features and labels are properly aligned\n",
    "        确保特征和标签正确对齐\n",
    "        \"\"\"\n",
    "        # Get common index\n",
    "        common_index = features.index.intersection(labels.index)\n",
    "        \n",
    "        if len(common_index) < len(features):\n",
    "            print(f\"⚠️ Warning: {len(features) - len(common_index)} samples dropped due to misalignment\")\n",
    "        \n",
    "        return features.loc[common_index], labels.loc[common_index]\n",
    "    \n",
    "    @staticmethod\n",
    "    def memory_efficient_loading(instruments: List[str], \n",
    "                               batch_size: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"Load data in batches to save memory\n",
    "        分批加载数据以节省内存\n",
    "        \"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        for i in range(0, len(instruments), batch_size):\n",
    "            batch = instruments[i:i+batch_size]\n",
    "            batch_data = D.features(\n",
    "                instruments=batch,\n",
    "                fields=['$close', '$volume'],\n",
    "                start_time='2024-01-01',\n",
    "                end_time='2024-01-31'\n",
    "            )\n",
    "            all_data.append(batch_data)\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if i % (batch_size * 5) == 0:\n",
    "                import gc\n",
    "                gc.collect()\n",
    "        \n",
    "        return pd.concat(all_data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def debug_model_predictions(model, dataset, sample_size: int = 10):\n",
    "        \"\"\"Debug model predictions\n",
    "        调试模型预测\n",
    "        \"\"\"\n",
    "        # Get sample data\n",
    "        sample_data = dataset.prepare('test', col_set=['feature', 'label']).head(sample_size)\n",
    "        \n",
    "        # Make predictions\n",
    "        X = sample_data.iloc[:, :-1]\n",
    "        y_true = sample_data.iloc[:, -1]\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = model.predict(X) if hasattr(model, 'predict') else None\n",
    "        \n",
    "        # Debug output\n",
    "        debug_df = pd.DataFrame({\n",
    "            'True': y_true,\n",
    "            'Predicted': y_pred,\n",
    "            'Error': y_pred - y_true if y_pred is not None else None\n",
    "        })\n",
    "        \n",
    "        print(\"Debug Output:\")\n",
    "        print(debug_df)\n",
    "        \n",
    "        # Check for common issues\n",
    "        if y_pred is not None:\n",
    "            if np.all(y_pred == y_pred[0]):\n",
    "                print(\"⚠️ Warning: Model predicting constant values!\")\n",
    "            if np.any(np.isnan(y_pred)):\n",
    "                print(\"⚠️ Warning: Model producing NaN predictions!\")\n",
    "            if np.any(np.isinf(y_pred)):\n",
    "                print(\"⚠️ Warning: Model producing infinite predictions!\")\n",
    "\n",
    "# Example usage / 使用示例\n",
    "best_practices = QlibBestPractices()\n",
    "\n",
    "# Create sample data with issues / 创建有问题的样本数据\n",
    "dates = pd.date_range('2024-01-01', '2024-01-31')\n",
    "problem_data = pd.DataFrame({\n",
    "    'feature1': np.random.randn(len(dates)),\n",
    "    'feature2': np.random.randn(len(dates)),\n",
    "    'label': np.random.randn(len(dates))\n",
    "}, index=dates)\n",
    "\n",
    "# Add some NaN values\n",
    "problem_data.iloc[5:8, 0] = np.nan\n",
    "problem_data.iloc[15:17, 1] = np.nan\n",
    "\n",
    "print(\"Original data with NaN:\")\n",
    "print(f\"  NaN count: {problem_data.isna().sum().sum()}\")\n",
    "\n",
    "# Fix missing data\n",
    "fixed_data = best_practices.handle_missing_data(problem_data, method='forward_fill')\n",
    "print(f\"\\nAfter fixing:\")\n",
    "print(f\"  NaN count: {fixed_data.isna().sum().sum()}\")\n",
    "\n",
    "# Avoid look-ahead bias\n",
    "safe_data = best_practices.avoid_look_ahead_bias(fixed_data, shift_days=1)\n",
    "print(f\"\\nAfter avoiding look-ahead bias:\")\n",
    "print(f\"  Data shape: {safe_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary / 总结\n",
    "\n",
    "### What we covered / 本章内容\n",
    "\n",
    "#### Part 1: Core Utilities / 核心工具\n",
    "- **Workflow Automation**: Pipeline creation and management / 管道创建和管理\n",
    "- **Experiment Management**: Tracking and comparing experiments / 跟踪和比较实验\n",
    "- **Data Utilities**: Advanced data processing tools / 高级数据处理工具\n",
    "- **Feature Engineering**: Comprehensive feature creation / 综合特征创建\n",
    "\n",
    "#### Part 2: Advanced Features / 高级功能\n",
    "- **Multi-Market Support**: Cross-market trading framework / 跨市场交易框架\n",
    "- **Online Learning**: Incremental model updates / 增量模型更新\n",
    "- **Meta-Learning**: Learning to learn strategies / 学习策略的策略\n",
    "- **AutoML Integration**: Automated model selection / 自动模型选择\n",
    "\n",
    "#### Part 3: Production Tools / 生产工具\n",
    "- **Real-time Trading**: Live trading system / 实盘交易系统\n",
    "- **Monitoring & Alerting**: Comprehensive monitoring / 综合监控\n",
    "- **Debugging & Profiling**: Performance optimization / 性能优化\n",
    "- **Deployment Tools**: Production deployment / 生产部署\n",
    "\n",
    "#### Part 4: Best Practices / 最佳实践\n",
    "- **Code Organization**: Project structure / 项目结构\n",
    "- **Performance Optimization**: Speed and memory / 速度和内存\n",
    "- **Common Pitfalls**: Solutions to frequent issues / 常见问题解决方案\n",
    "- **Tips and Tricks**: Pro techniques / 专业技巧\n",
    "\n",
    "### Key Takeaways / 关键要点\n",
    "\n",
    "1. **Automation is Key**: Automate repetitive tasks / 自动化重复任务\n",
    "2. **Monitor Everything**: Track all metrics / 跟踪所有指标\n",
    "3. **Handle Edge Cases**: Robust error handling / 稳健的错误处理\n",
    "4. **Optimize Performance**: Profile and optimize / 分析和优化\n",
    "5. **Think Production**: Design for deployment / 为部署而设计\n",
    "\n",
    "### Resources / 资源\n",
    "\n",
    "- [Qlib Documentation](https://qlib.readthedocs.io/)\n",
    "- [Qlib GitHub](https://github.com/microsoft/qlib)\n",
    "- [Qlib Examples](https://github.com/microsoft/qlib/tree/main/examples)\n",
    "- [Community Forum](https://github.com/microsoft/qlib/discussions)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed the comprehensive Qlib tutorial series!\n",
    "\n",
    "**恭喜！** 您已完成全面的Qlib教程系列！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}